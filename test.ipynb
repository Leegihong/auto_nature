{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 INTRODUCTION: Largelanguagemodels(LLMs),representedbyChatGPT(OpenAI,2022),havebecomepowerful general-purposetasksolvers,capableofassistingpeopleindailylifethroughconversationalinter- actions. However, most LLMs currently only support text-based interactions, which limits their applicationinscenarioswheretextinputandoutputarenotideal. Recently,theemergenceofGPT- 4o(OpenAI,2024)hasmadeitpossibletointeractwithLLMsthroughspeech,respondingtouser’s instruction with extremely low latency and significantly enhancing the user experience. However, thereisstillalackofexplorationintheopen-sourcecommunityonbuildingsuchspeechinteraction modelsbasedonLLMs. Therefore,howtoachievelow-latencyandhigh-qualityspeechinteraction withLLMsisapressingchallengethatneedstobeaddressed. The simplest way to enable speech interaction with LLMs is through a cascaded system based on automaticspeechrecognition(ASR)andtext-to-speech(TTS)models,wheretheASRmodeltran- scribestheuser’sspeechinstructionintotext, andtheTTSmodelsynthesizestheLLM’sresponse intospeech. However, sincethecascadedsystemsequentiallyoutputsthetranscribedtext, textre- sponse, and speech response, the overall system tends to have higher latency. In contrast, some multimodal speech-language models have been proposed (Zhang et al., 2023; Rubenstein et al., 2023),whichdiscretizespeechintotokensandextendtheLLM’svocabularytosupportspeechin- putandoutput. Suchspeech-languagemodelstheoreticallycangeneratespeechresponsesdirectly ∗CorrespondingAuthor:YangFeng. 1Codeandmodelareavailableathttps://github.com/ictnlp/LLaMA-Omni. 1 4202 peS 01 ]LC.sc[ 1v66660.9042:viXra Preprint simultaneous LLaMA-Omni generation (Hey! Can you give me some Certainly! Writing a high-quality NLP paper requires … advices on writing NLP papers?) Figure1:LLaMA-Omnicansimultaneouslygeneratetextandspeechresponsesbasedonthespeech instruction,withextremelylowresponselatency. fromspeechinstructionswithoutproducingintermediatetext,therebyachievingextremelylowre- sponse latency. However, in practice, direct speech-to-speech generation can be challenging due to thecomplex mappinginvolved, soit is commonto generateintermediate textto achieve higher generationquality(Zhangetal.,2023),althoughthissacrificessomeresponselatency. Inthispaper,weproposeanovelmodelarchitecture,LLaMA-Omni,whichenableslow-latencyand high-qualityinteractionwithLLMs. LLaMA-Omniconsistsofaspeechencoder,aspeechadaptor, an LLM, and a streaming speech decoder. The user’s speech instruction is encoded by the speech encoder followed by the speech adaptor, and then input into the LLM. The LLM decodes the text response directly from the speech instruction, without first transcribing the speech into text. The speechdecoderisanon-autoregressive(NAR)streamingTransformer(Maetal.,2023),whichtakes theoutputhiddenstatesoftheLLMasinputandusesconnectionisttemporalclassification(CTC; Gravesetal.,2006a)topredictthesequenceofdiscreteunitscorrespondingtothespeechresponse. Duringinference, astheLLM autoregressivelygeneratesthetextresponse, thespeech decodersi- multaneouslygeneratesthecorrespondingdiscreteunits. Tobetteralignwiththecharacteristicsof speechinteractionscenarios,weconstructadatasetnamedInstructS2S-200Kbyrewritingexisting text instruction data and performing speech synthesis. Experimental results show that LLaMA- Omnicansimultaneouslygeneratehigh-qualitytextandspeechresponseswithalatencyaslowas 226ms. Additionally,comparedtopreviousspeech-languagemodelslikeSpeechGPT(Zhangetal., 2023),LLaMA-Omnisignificantlyreducestherequiredtrainingdataandcomputationalresources, enablingtheefficientdevelopmentofpowerfulspeechinteractionmodelsbasedonthelatestLLMs. \n",
      "\n",
      "2 MODEL: LLAMA-OMNI: In this section, we introduce the model architecture of LLaMA-Omni. As shown in Figure 2, it consistsofaspeechencoder,aspeechadaptor,anLLM,andaspeechdecoder. Wedenotetheuser’s speechinstruction,textresponse,andspeechresponseasXS,YT,andYS respectively. 2.1 SPEECHENCODER WeusetheencoderofWhisper-large-v32(Radfordetal.,2023)asthespeechencoder . Whisperis E ageneral-purposespeechrecognitionmodeltrainedonalargeamountofaudiodata,anditsencoder iscapableofextractingmeaningfulrepresentationsfromspeech. Specifically,fortheuser’sspeech instructionXS,theencodedspeechrepresentationisgivenbyH= (XS),whereH=[h ,...,h ] 1 N E isthespeechrepresentationsequenceoflengthN. Wekeepthespeechencoder’sparametersfrozen throughouttheentiretrainingprocess. 2.2 SPEECHADAPTOR To enable the LLM to comprehend the input speech, we incorporate a trainable speech adaptor A that maps the speech representations into the embedding space of the LLM. Following Ma et al. (2024c),ourspeechadaptorfirstdownsamplesthespeechrepresentationsHtoreducethesequence length. Specifically,everykconsecutiveframesareconcatenatedalongthefeaturedimension: H′ = h′,...,h′ ,whereh′ = h h h . (1) 1 ⌊N/k⌋ i k×(i−1)+1 ⊕ k×(i−1)+2 ⊕···⊕ k×i (cid:104) (cid:105) (cid:2) (cid:3) 2https://huggingface.co/openai/whisper-large-v3 2 Preprint latency … Large Language Model Vocoder A ✏ ✏ ✏ ✏ 65 65 27 ✏ 98 ✏ 11 11 53 2 … Speech Adaptor <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"BBBBAAAAeeeeVVVVOOOOBBBBCCCC5555OOOObbbbWWWWqqqqGGGGCCCCFFFFkkkk55552222KKKKllllPPPP7777hhhhccccwwwwRRRRgggg====\"\"\"\">>>>AAAAAAAAAAAABBBB6666HHHHiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ3333UUUUrrrr1111qqqq////qqqqhhhh66669999LLLLBBBBbbbbBBBBUUUU0000llllEEEE0000GGGGPPPPVVVViiii8888ccccWWWW7777AAAAeeee0000ooooWWWWyyyy2222kkkk3333bbbbttttZZZZhhhhNNNN2222NNNN0000IIIIJJJJ////QQQQVVVVeeeePPPPCCCCjjjjiiii1111ZZZZ////kkkkzzzzXXXX////jjjjttttssss1111BBBBWWWWxxxx8888MMMMPPPPNNNN6666bbbbYYYYWWWWZZZZeeeekkkkAAAAiiiiuuuujjjjeeeetttt++++OOOO4444WWWW11119999YYYY3333NNNNrrrreeeeJJJJ2222aaaaWWWWdddd3333bbbb////++++ggggffffHHHHjjjjUUUU0000nnnnGGGGqqqqGGGGDDDDZZZZZZZZLLLLGGGGLLLLVVVVCCCCaaaahhhhGGGGwwwwSSSSUUUU2222DDDDTTTTccccCCCCOOOO4444llllCCCCGGGGggggUUUUCCCC22228888HHHH4444bbbbuuuuaaaa3333nnnn1111BBBBppppHHHHssssssssHHHHMMMM0000nnnnQQQQjjjj++++hhhhQQQQ8888ppppAAAAzzzzaaaaqqqqzzzzUUUUuuuuOOOOmmmmXXXXKKKK22227777VVVVnnnnYYYYOOOOssssEEEEiiii8888nnnnFFFFcccchhhhRRRR77775555eeee////eeeeooooOOOOYYYYppppRRRRFFFFKKKKwwwwwwwwTTTTVVVVuuuuuuuuuuuu5555iiiiffffEEEEzzzzqqqqggggxxxxnnnnAAAAqqqqeeeellllXXXXqqqqooooxxxxooooWWWWxxxxMMMMhhhh9999iiii1111VVVVNNNNIIIIIIIIttttZZZZ////NNNNDDDD55552222SSSSMMMM6666ssssMMMMSSSSBBBBggggrrrrWWWW9999KKKKQQQQuuuuffffpppp7777IIIIqqqqOOOORRRR1111ppppMMMMoooossssJJJJ0000RRRRNNNNSSSSOOOO99997777MMMM3333EEEE////7777xxxxuuuuaaaassssJJJJrrrrPPPP++++MMMMyyyySSSSQQQQ1111KKKKttttllllggggUUUUppppooooKKKKYYYYmmmmMMMMyyyy++++JJJJggggOOOOuuuukkkkBBBBkkkkxxxxssssYYYYQQQQyyyyxxxxeeee2222tttthhhhIIII2222oooooooosssszzzzYYYYbbbbEEEEoooo2222BBBBGGGG////55555555VVVVXXXXSSSSuuuuqqqqhhhh6666bbbbttttVVVVrrrrXXXXFFFFZZZZqqqqtttt3333kkkkccccRRRRTTTTiiiiBBBBUUUUzzzzggggHHHHDDDD66666666ggggBBBBvvvvddddQQQQhhhhyyyyYYYYwwwwQQQQHHHHiiiiGGGGVVVV3333hhhhzzzzHHHHpppp0000XXXX5555999933335555WWWWLLLLQQQQWWWWnnnnHHHHzzzzmmmmGGGGPPPP7777AAAA++++ffffwwwwBBBBkkkktttt2222MMMMxxxxQQQQ========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ssssssssxxxxxxxxssssccccrrrrssssccccFFFFDDDDuuuuEEEELLLLeeeeuuuuuuuu////++++9999OOOO0000MMMMDDDDmmmmKKKKIIII====\"\"\"\">>>>AAAAAAAAAAAABBBB77773333iiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEOOOOyyyyNNNNrrrrxxxxhhhhffffUUUUYYYY9999eeeeBBBBooooPPPPggggKKKKeeeeyyyyKKKKooooMMMMeeeeggggFFFF44448888RRRRzzzzAAAAOOOOSSSSJJJJccccxxxxOOOOeeeeppppMMMMhhhhsssszzzzPPPPrrrrzzzzKKKKwwwwQQQQQQQQnnnn7777CCCCiiiiwwwwddddFFFFvvvvPPPPoooo77773333vvvvwwwwbbbbJJJJ8888kkkkeeeeNNNNLLLLGGGGggggooooaaaajjjjqqqqpppprrrrssssrrrrSSSSggggUUUU33331111vvvveeee////vvvvccccLLLLaaaa++++ssssbbbbmmmmVVVVnnnnGGGG7777ttttLLLLOOOO7777tttt33339999QQQQPPPPjjjjxxxxqqqqGGGGppppVVVVpppphhhhgggg2222mmmmhhhhNNNNLLLLttttiiiiBBBBooooUUUUXXXXGGGGLLLLDDDDcccciiiiuuuuwwwwnnnnWWWWqqqqkkkkSSSSSSSSSSSSwwwwFFFFYYYY1111uuuuZZZZ33337777rrrrCCCCbbbbXXXXhhhhSSSSjjjj7777YYYYccccYYYYpppphhhhQQQQggggeeeeSSSSxxxx5555xxxxRRRR66666666RRRR2222FFFF1111PPPPDDDDhhhhZZZZKKKK9999ccccssssWWWWvvvv++++nnnnOOOOQQQQVVVVRRRRLLLLkkkkppppAAAAIIII55556666rrrr3333yyyyVVVV7777eeeevvvvWWWWJJJJaaaaggggttttEEEExxxxQQQQYYYYzzzzqqqqBBBBnnnn9999ppppwwwwQQQQrrrrXXXXllllTTTTOOOOCCCC00001111MMMM0000MMMMppppppppSSSSNNNN6666AAAAAAAA7777jjjjkkkkqqqqaaaaooooAAAAkkkknnnn88883333uuuunnnn5555MMMMwwwwppppffffRRRRIIIIrrrr7777UUUUppppaaaaMMMMlllldddd////TTTT0000xxxxooooYYYYsssswwww4444iiiiVVVVxxxxnnnnQQQQuuuu3333QQQQLLLLHHHHsssszzzz8888TTTT++++vvvvkkkk9999nnnn4444OOOOppppxxxxwwwwmmmmWWWWYYYYWWWWJJJJVVVVssssssssiiiijjjjNNNNBBBBrrrrCCCCKKKKzzzz55550000mmmmffffaaaa2222RRRRWWWWjjjjBBBB2222hhhhTTTTHHHHNNNN3333KKKK2222FFFFDDDDqqqqiiiimmmmzzzzLLLLqqqqKKKKSSSSCCCCyyyyFFFFYYYYffffnnnnmmmmVVVVNNNNCCCC++++qqqqggggVVVV8888NNNN7777iiii8888rrrrttttZZZZssss8888jjjjiiiiKKKKccccwwwwCCCCmmmmccccQQQQwwwwBBBBXXXXUUUUIIIIMMMM7777qqqqEEEEMMMMDDDDGGGGAAAAhhhh4444hhhhlllldddd44448888xxxx66669999FFFF++++////dddd++++1111iiii0000FFFFrrrrxxxx88885555hhhhjjjj++++wwwwPPPPvvvv8888AAAAUUUUzzzzRRRRkkkkCCCCAAAA====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ssssssssxxxxxxxxssssccccrrrrssssccccFFFFDDDDuuuuEEEELLLLeeeeuuuuuuuu////++++9999OOOO0000MMMMDDDDmmmmKKKKIIII====\"\"\"\">>>>AAAAAAAAAAAABBBB77773333iiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEOOOOyyyyNNNNrrrrxxxxhhhhffffUUUUYYYY9999eeeeBBBBooooPPPPggggKKKKeeeeyyyyKKKKooooMMMMeeeeggggFFFF44448888RRRRzzzzAAAAOOOOSSSSJJJJccccxxxxOOOOeeeeppppMMMMhhhhsssszzzzPPPPrrrrzzzzKKKKwwwwQQQQQQQQnnnn7777CCCCiiiiwwwwddddFFFFvvvvPPPPoooo77773333vvvvwwwwbbbbJJJJ8888kkkkeeeeNNNNLLLLGGGGggggooooaaaajjjjqqqqpppprrrrssssrrrrSSSSggggUUUU33331111vvvveeee////vvvvccccLLLLaaaa++++ssssbbbbmmmmVVVVnnnnGGGG7777ttttLLLLOOOO7777tttt33339999QQQQPPPPjjjjxxxxqqqqGGGGppppVVVVpppphhhhgggg2222mmmmhhhhNNNNLLLLttttiiiiBBBBooooUUUUXXXXGGGGLLLLDDDDcccciiiiuuuuwwwwnnnnWWWWqqqqkkkkSSSSSSSSSSSSwwwwFFFFYYYY1111uuuuZZZZ33337777rrrrCCCCbbbbXXXXhhhhSSSSjjjj7777YYYYccccYYYYpppphhhhQQQQggggeeeeSSSSxxxx5555xxxxRRRR66666666RRRR2222FFFF1111PPPPDDDDhhhhZZZZKKKK9999ccccssssWWWWvvvv++++nnnnOOOOQQQQVVVVRRRRLLLLkkkkppppAAAAIIII55556666rrrr3333yyyyVVVV7777eeeevvvvWWWWJJJJaaaaggggttttEEEExxxxQQQQYYYYzzzzqqqqBBBBnnnn9999ppppwwwwQQQQrrrrXXXXllllTTTTOOOOCCCC00001111MMMM0000MMMMppppppppSSSSNNNN6666AAAAAAAA7777jjjjkkkkqqqqaaaaooooAAAAkkkknnnn88883333uuuunnnn5555MMMMwwwwppppffffRRRRIIIIrrrr7777UUUUppppaaaaMMMMlllldddd////TTTT0000xxxxooooYYYYsssswwww4444iiiiVVVVxxxxnnnnQQQQuuuu3333QQQQLLLLHHHHsssszzzz8888TTTT++++vvvvkkkk9999nnnn4444OOOOppppxxxxwwwwmmmmWWWWYYYYWWWWJJJJVVVVssssssssiiiijjjjNNNNBBBBrrrrCCCCKKKKzzzz55550000mmmmffffaaaa2222RRRRWWWWjjjjBBBB2222hhhhTTTTHHHHNNNN3333KKKK2222FFFFDDDDqqqqiiiimmmmzzzzLLLLqqqqKKKKSSSSCCCCyyyyFFFFYYYYffffnnnnmmmmVVVVNNNNCCCC++++qqqqggggVVVV8888NNNN7777iiii8888rrrrttttZZZZssss8888jjjjiiiiKKKKccccwwwwCCCCmmmmccccQQQQwwwwBBBBXXXXUUUUIIIIMMMM7777qqqqEEEEMMMMDDDDGGGGAAAAhhhh4444hhhhlllldddd44448888xxxx66669999FFFF++++////dddd++++1111iiii0000FFFFrrrrxxxx88885555hhhhjjjj++++wwwwPPPPvvvv8888AAAAUUUUzzzzRRRRkkkkCCCCAAAA====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ssssssssxxxxxxxxssssccccrrrrssssccccFFFFDDDDuuuuEEEELLLLeeeeuuuuuuuu////++++9999OOOO0000MMMMDDDDmmmmKKKKIIII====\"\"\"\">>>>AAAAAAAAAAAABBBB77773333iiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEOOOOyyyyNNNNrrrrxxxxhhhhffffUUUUYYYY9999eeeeBBBBooooPPPPggggKKKKeeeeyyyyKKKKooooMMMMeeeeggggFFFF44448888RRRRzzzzAAAAOOOOSSSSJJJJccccxxxxOOOOeeeeppppMMMMhhhhsssszzzzPPPPrrrrzzzzKKKKwwwwQQQQQQQQnnnn7777CCCCiiiiwwwwddddFFFFvvvvPPPPoooo77773333vvvvwwwwbbbbJJJJ8888kkkkeeeeNNNNLLLLGGGGggggooooaaaajjjjqqqqpppprrrrssssrrrrSSSSggggUUUU33331111vvvveeee////vvvvccccLLLLaaaa++++ssssbbbbmmmmVVVVnnnnGGGG7777ttttLLLLOOOO7777tttt33339999QQQQPPPPjjjjxxxxqqqqGGGGppppVVVVpppphhhhgggg2222mmmmhhhhNNNNLLLLttttiiiiBBBBooooUUUUXXXXGGGGLLLLDDDDcccciiiiuuuuwwwwnnnnWWWWqqqqkkkkSSSSSSSSSSSSwwwwFFFFYYYY1111uuuuZZZZ33337777rrrrCCCCbbbbXXXXhhhhSSSSjjjj7777YYYYccccYYYYpppphhhhQQQQggggeeeeSSSSxxxx5555xxxxRRRR66666666RRRR2222FFFF1111PPPPDDDDhhhhZZZZKKKK9999ccccssssWWWWvvvv++++nnnnOOOOQQQQVVVVRRRRLLLLkkkkppppAAAAIIII55556666rrrr3333yyyyVVVV7777eeeevvvvWWWWJJJJaaaaggggttttEEEExxxxQQQQYYYYzzzzqqqqBBBBnnnn9999ppppwwwwQQQQrrrrXXXXllllTTTTOOOOCCCC00001111MMMM0000MMMMppppppppSSSSNNNN6666AAAAAAAA7777jjjjkkkkqqqqaaaaooooAAAAkkkknnnn88883333uuuunnnn5555MMMMwwwwppppffffRRRRIIIIrrrr7777UUUUppppaaaaMMMMlllldddd////TTTT0000xxxxooooYYYYsssswwww4444iiiiVVVVxxxxnnnnQQQQuuuu3333QQQQLLLLHHHHsssszzzz8888TTTT++++vvvvkkkk9999nnnn4444OOOOppppxxxxwwwwmmmmWWWWYYYYWWWWJJJJVVVVssssssssiiiijjjjNNNNBBBBrrrrCCCCKKKKzzzz55550000mmmmffffaaaa2222RRRRWWWWjjjjBBBB2222hhhhTTTTHHHHNNNN3333KKKK2222FFFFDDDDqqqqiiiimmmmzzzzLLLLqqqqKKKKSSSSCCCCyyyyFFFFYYYYffffnnnnmmmmVVVVNNNNCCCC++++qqqqggggVVVV8888NNNN7777iiii8888rrrrttttZZZZssss8888jjjjiiiiKKKKccccwwwwCCCCmmmmccccQQQQwwwwBBBBXXXXUUUUIIIIMMMM7777qqqqEEEEMMMMDDDDGGGGAAAAhhhh4444hhhhlllldddd44448888xxxx66669999FFFF++++////dddd++++1111iiii0000FFFFrrrrxxxx88885555hhhhjjjj++++wwwwPPPPvvvv8888AAAAUUUUzzzzRRRRkkkkCCCCAAAA====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ssssssssxxxxxxxxssssccccrrrrssssccccFFFFDDDDuuuuEEEELLLLeeeeuuuuuuuu////++++9999OOOO0000MMMMDDDDmmmmKKKKIIII====\"\"\"\">>>>AAAAAAAAAAAABBBB77773333iiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEOOOOyyyyNNNNrrrrxxxxhhhhffffUUUUYYYY9999eeeeBBBBooooPPPPggggKKKKeeeeyyyyKKKKooooMMMMeeeeggggFFFF44448888RRRRzzzzAAAAOOOOSSSSJJJJccccxxxxOOOOeeeeppppMMMMhhhhsssszzzzPPPPrrrrzzzzKKKKwwwwQQQQQQQQnnnn7777CCCCiiiiwwwwddddFFFFvvvvPPPPoooo77773333vvvvwwwwbbbbJJJJ8888kkkkeeeeNNNNLLLLGGGGggggooooaaaajjjjqqqqpppprrrrssssrrrrSSSSggggUUUU33331111vvvveeee////vvvvccccLLLLaaaa++++ssssbbbbmmmmVVVVnnnnGGGG7777ttttLLLLOOOO7777tttt33339999QQQQPPPPjjjjxxxxqqqqGGGGppppVVVVpppphhhhgggg2222mmmmhhhhNNNNLLLLttttiiiiBBBBooooUUUUXXXXGGGGLLLLDDDDcccciiiiuuuuwwwwnnnnWWWWqqqqkkkkSSSSSSSSSSSSwwwwFFFFYYYY1111uuuuZZZZ33337777rrrrCCCCbbbbXXXXhhhhSSSSjjjj7777YYYYccccYYYYpppphhhhQQQQggggeeeeSSSSxxxx5555xxxxRRRR66666666RRRR2222FFFF1111PPPPDDDDhhhhZZZZKKKK9999ccccssssWWWWvvvv++++nnnnOOOOQQQQVVVVRRRRLLLLkkkkppppAAAAIIII55556666rrrr3333yyyyVVVV7777eeeevvvvWWWWJJJJaaaaggggttttEEEExxxxQQQQYYYYzzzzqqqqBBBBnnnn9999ppppwwwwQQQQrrrrXXXXllllTTTTOOOOCCCC00001111MMMM0000MMMMppppppppSSSSNNNN6666AAAAAAAA7777jjjjkkkkqqqqaaaaooooAAAAkkkknnnn88883333uuuunnnn5555MMMMwwwwppppffffRRRRIIIIrrrr7777UUUUppppaaaaMMMMlllldddd////TTTT0000xxxxooooYYYYsssswwww4444iiiiVVVVxxxxnnnnQQQQuuuu3333QQQQLLLLHHHHsssszzzz8888TTTT++++vvvvkkkk9999nnnn4444OOOOppppxxxxwwwwmmmmWWWWYYYYWWWWJJJJVVVVssssssssiiiijjjjNNNNBBBBrrrrCCCCKKKKzzzz55550000mmmmffffaaaa2222RRRRWWWWjjjjBBBB2222hhhhTTTTHHHHNNNN3333KKKK2222FFFFDDDDqqqqiiiimmmmzzzzLLLLqqqqKKKKSSSSCCCCyyyyFFFFYYYYffffnnnnmmmmVVVVNNNNCCCC++++qqqqggggVVVV8888NNNN7777iiii8888rrrrttttZZZZssss8888jjjjiiiiKKKKccccwwwwCCCCmmmmccccQQQQwwwwBBBBXXXXUUUUIIIIMMMM7777qqqqEEEEMMMMDDDDGGGGAAAAhhhh4444hhhhlllldddd44448888xxxx66669999FFFF++++////dddd++++1111iiii0000FFFFrrrrxxxx88885555hhhhjjjj++++wwwwPPPPvvvv8888AAAAUUUUzzzzRRRRkkkkCCCCAAAA====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ssssssssxxxxxxxxssssccccrrrrssssccccFFFFDDDDuuuuEEEELLLLeeeeuuuuuuuu////++++9999OOOO0000MMMMDDDDmmmmKKKKIIII====\"\"\"\">>>>AAAAAAAAAAAABBBB77773333iiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEOOOOyyyyNNNNrrrrxxxxhhhhffffUUUUYYYY9999eeeeBBBBooooPPPPggggKKKKeeeeyyyyKKKKooooMMMMeeeeggggFFFF44448888RRRRzzzzAAAAOOOOSSSSJJJJccccxxxxOOOOeeeeppppMMMMhhhhsssszzzzPPPPrrrrzzzzKKKKwwwwQQQQQQQQnnnn7777CCCCiiiiwwwwddddFFFFvvvvPPPPoooo77773333vvvvwwwwbbbbJJJJ8888kkkkeeeeNNNNLLLLGGGGggggooooaaaajjjjqqqqpppprrrrssssrrrrSSSSggggUUUU33331111vvvveeee////vvvvccccLLLLaaaa++++ssssbbbbmmmmVVVVnnnnGGGG7777ttttLLLLOOOO7777tttt33339999QQQQPPPPjjjjxxxxqqqqGGGGppppVVVVpppphhhhgggg2222mmmmhhhhNNNNLLLLttttiiiiBBBBooooUUUUXXXXGGGGLLLLDDDDcccciiiiuuuuwwwwnnnnWWWWqqqqkkkkSSSSSSSSSSSSwwwwFFFFYYYY1111uuuuZZZZ33337777rrrrCCCCbbbbXXXXhhhhSSSSjjjj7777YYYYccccYYYYpppphhhhQQQQggggeeeeSSSSxxxx5555xxxxRRRR66666666RRRR2222FFFF1111PPPPDDDDhhhhZZZZKKKK9999ccccssssWWWWvvvv++++nnnnOOOOQQQQVVVVRRRRLLLLkkkkppppAAAAIIII55556666rrrr3333yyyyVVVV7777eeeevvvvWWWWJJJJaaaaggggttttEEEExxxxQQQQYYYYzzzzqqqqBBBBnnnn9999ppppwwwwQQQQrrrrXXXXllllTTTTOOOOCCCC00001111MMMM0000MMMMppppppppSSSSNNNN6666AAAAAAAA7777jjjjkkkkqqqqaaaaooooAAAAkkkknnnn88883333uuuunnnn5555MMMMwwwwppppffffRRRRIIIIrrrr7777UUUUppppaaaaMMMMlllldddd////TTTT0000xxxxooooYYYYsssswwww4444iiiiVVVVxxxxnnnnQQQQuuuu3333QQQQLLLLHHHHsssszzzz8888TTTT++++vvvvkkkk9999nnnn4444OOOOppppxxxxwwwwmmmmWWWWYYYYWWWWJJJJVVVVssssssssiiiijjjjNNNNBBBBrrrrCCCCKKKKzzzz55550000mmmmffffaaaa2222RRRRWWWWjjjjBBBB2222hhhhTTTTHHHHNNNN3333KKKK2222FFFFDDDDqqqqiiiimmmmzzzzLLLLqqqqKKKKSSSSCCCCyyyyFFFFYYYYffffnnnnmmmmVVVVNNNNCCCC++++qqqqggggVVVV8888NNNN7777iiii8888rrrrttttZZZZssss8888jjjjiiiiKKKKccccwwwwCCCCmmmmccccQQQQwwwwBBBBXXXXUUUUIIIIMMMM7777qqqqEEEEMMMMDDDDGGGGAAAAhhhh4444hhhhlllldddd44448888xxxx66669999FFFF++++////dddd++++1111iiii0000FFFFrrrrxxxx88885555hhhhjjjj++++wwwwPPPPvvvv8888AAAAUUUUzzzzRRRRkkkkCCCCAAAA====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ssssssssxxxxxxxxssssccccrrrrssssccccFFFFDDDDuuuuEEEELLLLeeeeuuuuuuuu////++++9999OOOO0000MMMMDDDDmmmmKKKKIIII====\"\"\"\">>>>AAAAAAAAAAAABBBB77773333iiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEOOOOyyyyNNNNrrrrxxxxhhhhffffUUUUYYYY9999eeeeBBBBooooPPPPggggKKKKeeeeyyyyKKKKooooMMMMeeeeggggFFFF44448888RRRRzzzzAAAAOOOOSSSSJJJJccccxxxxOOOOeeeeppppMMMMhhhhsssszzzzPPPPrrrrzzzzKKKKwwwwQQQQQQQQnnnn7777CCCCiiiiwwwwddddFFFFvvvvPPPPoooo77773333vvvvwwwwbbbbJJJJ8888kkkkeeeeNNNNLLLLGGGGggggooooaaaajjjjqqqqpppprrrrssssrrrrSSSSggggUUUU33331111vvvveeee////vvvvccccLLLLaaaa++++ssssbbbbmmmmVVVVnnnnGGGG7777ttttLLLLOOOO7777tttt33339999QQQQPPPPjjjjxxxxqqqqGGGGppppVVVVpppphhhhgggg2222mmmmhhhhNNNNLLLLttttiiiiBBBBooooUUUUXXXXGGGGLLLLDDDDcccciiiiuuuuwwwwnnnnWWWWqqqqkkkkSSSSSSSSSSSSwwwwFFFFYYYY1111uuuuZZZZ33337777rrrrCCCCbbbbXXXXhhhhSSSSjjjj7777YYYYccccYYYYpppphhhhQQQQggggeeeeSSSSxxxx5555xxxxRRRR66666666RRRR2222FFFF1111PPPPDDDDhhhhZZZZKKKK9999ccccssssWWWWvvvv++++nnnnOOOOQQQQVVVVRRRRLLLLkkkkppppAAAAIIII55556666rrrr3333yyyyVVVV7777eeeevvvvWWWWJJJJaaaaggggttttEEEExxxxQQQQYYYYzzzzqqqqBBBBnnnn9999ppppwwwwQQQQrrrrXXXXllllTTTTOOOOCCCC00001111MMMM0000MMMMppppppppSSSSNNNN6666AAAAAAAA7777jjjjkkkkqqqqaaaaooooAAAAkkkknnnn88883333uuuunnnn5555MMMMwwwwppppffffRRRRIIIIrrrr7777UUUUppppaaaaMMMMlllldddd////TTTT0000xxxxooooYYYYsssswwww4444iiiiVVVVxxxxnnnnQQQQuuuu3333QQQQLLLLHHHHsssszzzz8888TTTT++++vvvvkkkk9999nnnn4444OOOOppppxxxxwwwwmmmmWWWWYYYYWWWWJJJJVVVVssssssssiiiijjjjNNNNBBBBrrrrCCCCKKKKzzzz55550000mmmmffffaaaa2222RRRRWWWWjjjjBBBB2222hhhhTTTTHHHHNNNN3333KKKK2222FFFFDDDDqqqqiiiimmmmzzzzLLLLqqqqKKKKSSSSCCCCyyyyFFFFYYYYffffnnnnmmmmVVVVNNNNCCCC++++qqqqggggVVVV8888NNNN7777iiii8888rrrrttttZZZZssss8888jjjjiiiiKKKKccccwwwwCCCCmmmmccccQQQQwwwwBBBBXXXXUUUUIIIIMMMM7777qqqqEEEEMMMMDDDDGGGGAAAAhhhh4444hhhhlllldddd44448888xxxx66669999FFFF++++////dddd++++1111iiii0000FFFFrrrrxxxx88885555hhhhjjjj++++wwwwPPPPvvvv8888AAAAUUUUzzzzRRRRkkkkCCCCAAAA====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Speech Encoder simultaneous Speech Decoder generation C … <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ZZZZJJJJhhhhKKKKooooggggBBBBffffCCCCBBBBwwwwQQQQssssppppaaaaLLLLXXXXDDDDYYYY2222HHHHoooo2222ffff////GGGGwwww====\"\"\"\">>>>AAAAAAAAAAAABBBB////XXXXiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvv++++NNNNiiii5555GGGGSSSSyyyyCCCCqqqq5555KKKKIIIIooooMMMMttttiiiiNNNNyyyy4444rrrr2222AAAAcccc0000ooooUUUUwwwwmmmmkkkk3333bbbboooo5555MMMMHHHHMMMMjjjjVVVVJJJJDDDD8888FFFFffffccccuuuuFFFFDDDDEEEErrrrffff////hhhhzzzzrrrr9999xxxx0000mmmmaaaahhhhrrrrQQQQccccGGGGDDDDuuuuffffccccyyyyzzzz1111zzzzvvvvEEEERRRRwwwwBBBBZZZZbbbb1111bbbbVVVVRRRRWWWWVVVVttttffffWWWWNNNN6666qqqqbbbbttttaaaa3333ttttnnnndddd00009999cccc////++++ggggqqqq++++JJJJUUUUUUUUttttaaaahhhhssssYYYYhhhhllll3333yyyyOOOOKKKKCCCCRRRR6666xxxxDDDDnnnnAAAAQQQQrrrrJJJJ9999IIIIRRRRkkkkJJJJPPPPssssJJJJ44443333aaaaRRRRVVVV++++777755555555JJJJxxxxeeeePPPPooooDDDDqqqqYYYYJJJJcccc0000MMMMyyyyiiiinnnnjjjjAAAAKKKKQQQQEEEEttttDDDDcccc0000jjjj55554444HHHH7777bbbbEEEEwwwwggggcccc0000IIIICCCCYYYYyyyy////IIIIWWWWnnnnkkkk++++NNNNOOOOttttWWWWwwww5555ooooBBBBLLLLxxxxOOOO7777JJJJHHHHVVVVUUUUoooojjjj00000000vvvvxxxxwwww////ppppmmmmnnnnIIIIIIIIqqqqCCCCCCCCKKKKDDDDWWWWwwwwrrrrQQQQTTTTccccjjjjEEEEjjjjggggVVVVLLLLCCCC88885555qqqqSSSSKKKKJJJJYYYYRRRROOOOyyyyIIIIggggNNNNNNNNIIII1111IIIIyyyyJJJJSSSSbbbbzzzzddddLLLLnnnn++++FFFFQQQQrrrrPPPPgggg5555iiiiqqqqVVVV8888EEEEeeeeKKKKbbbb++++3333sssshhhhIIIIqqqqNNNNQQQQ00009999PPPPRRRRkkkkEEEEVVVVEEEEtttteeeeooooXXXX4444nnnnzzzzddddIIIIIIIIbbbbhhhhyyyyMMMMxxxx4444llllKKKKbbbbCCCCIIIIzzzzgggg8888FFFFqqqqccccAAAAQQQQ44446666IIIIKKKK7777HHHHPPPPJJJJKKKKIIIIiiiippppJJJJooooRRRRKKKKrrrrrrrrNNNNiiiiOOOOiiiiaaaaSSSSUUUUNNNNCCCCFFFF1111XXXXQQQQJJJJ9999uuuuKKKKXXXXllll0000nnnn3333vvvvGGGGFFFFbbbbDDDDffffvvvv2222ooootttt66668888LLLLuuuuuuuuoooooooommmmNNNN0000ggggssss6666QQQQjjjjSSSS5555RRRREEEE99992222ggggNNNNuuuuooooggggiiiihhhh7777RRRRMMMM3333ppppFFFFbbbb8888aaaaTTTT8888WWWWKKKK8888GGGGxxxx////zzzz0000YYYYppppRRRR7777hhhhyyyyiiiiPPPPzzzzAAAA++++ffffwwwwAAAA7777GGGGppppWWWW2222<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Upsample b C Certainly! Writing a high … <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"YYYYuuuuwwwwffff++++5555SSSSXXXXWWWWbbbb0000MMMMOOOOGGGGSSSSTTTTllllIIIISSSSCCCCAAAA77775555qqqqqqqqiiii8888====\"\"\"\">>>>AAAAAAAAAAAABBBB8888XXXXiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFLLLL3333xxxxWWWWeeeeuuuurrrr6666ttttLLLLNNNNYYYYBBBBFFFFccccllllUUUUQQQQEEEEXXXXRRRRaaaa7777ccccVVVVnnnnBBBBPPPPrrrrAAAANNNNZZZZTTTTKKKK9999aaaaYYYYddddOOOOJJJJmmmmFFFFmmmmIIIIppppTTTTQQQQvvvv3333DDDDjjjjQQQQhhhhGGGG3333////oooo00007777////8888ZZZZJJJJmmmm4444WWWW2222HHHHhhhhgggg4444nnnnHHHHMMMMvvvvcccc++++4444JJJJEEEEssssGGGG1111ccccdddd1111vvvvZZZZ222211119999YYYY3333NNNNrrrruuuu7777RRRRTTTT3333tttt3333bbbbPPPPzzzziiiissssHHHHBBBB22223333ddddZZZZwwwwqqqqhhhhiiii0000WWWWiiii1111hhhh1111AAAA6666ppppRRRRccccIIIIkkkkttttwwww44443333AAAAbbbbqqqqKKKKQQQQRRRRooooHHHHAAAATTTTjjjjBBBBpppp5555HHHH7777nnnnCCCCZZZZXXXXmmmmssssXXXXwwwwwwww0000wwwwTTTT9999iiiiIIII4444kkkkDDDDzzzzmmmmjjjjxxxxkkkkqqqqPPPP////YYYYiiiiaaaaccccRRRRBBBBmmmmjjjjddddmmmmggggUUUUnnnnVVVVrrrr7777hhhhxxxxkkkkllllXXXXggggFFFFqqqqUUUUKKKKBBBB5555qqqqDDDDyyyy1111RRRR////GGGGLLLLIIII1111QQQQGGGGiiiiaaaaoooo1111jjjj3333PPPPTTTTYYYYyyyyffffUUUUWWWWUUUU4444EEEEzzzzggggrrrr99991111OOOONNNNCCCCWWWWUUUUTTTTOOOOssssKKKKeeeeppppZZZZJJJJGGGGqqqqPPPP1111ssssnnnnnnnnhhhhGGGGzzzzqqqq0000yyyyJJJJGGGGGGGGssss7777JJJJOOOOGGGGzzzzNNNNXXXXffffGGGGxxxxmmmmNNNNttttJJJJ5555GGGGggggZZZZ3333MMMMEEEE++++ppppllllLLLLxxxxffff////88883333qqqqppppCCCCWWWW////8888jjjjMMMMsssskkkkNNNNSSSSjjjjZZZZ4444qqqqMMMMwwwwFFFFccccTTTTEEEEJJJJDDDD++++ffffDDDDLLLLllllCCCCZZZZssssTTTTUUUUEEEEssssooooUUUUtttt1111kkkkJJJJGGGG1111NNNNFFFFmmmmbbbbEEEEllllllllWWWW0000JJJJ3333vvvvLLLLJJJJqqqq6666RRRR9999WWWWffffPPPPccccmmmmnnnndddd////VVVVaaaa3333ffffFFFFnnnnWWWWUUUU4444BBBBTTTTOOOO4444AAAAIIII8888uuuuIIIIYYYY66663333EEEEEEEETTTTWWWWssssBBBBAAAAwwwwjjjjOOOO8888wwwwppppuuuujjjjnnnnRRRRffffnnnn3333ffffllllYYYYjjjjKKKK44445555xxxxcccc4444JJJJ////IIIIHHHHzzzz++++QQQQOOOOssssssssJJJJDDDDnnnn<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Large Language Model S <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"aaaa0000vvvvIIIIHHHHoooozzzzkkkkjjjj3333MMMMeeeebbbbWWWWDDDDDDDDSSSSZZZZEEEECCCCQQQQ8888FFFFkkkkwwwwMMMMYYYY====\"\"\"\">>>>AAAAAAAAAAAABBBB8888XXXXiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAFFFFHHHHyyyyppppXXXX7777VVVV++++VVVVTTTT11116666WWWWSSSSyyyyCCCCpppp5555KKKKIIIIooooMMMMeeeeiiiiFFFF44448888VVVVbbbbSSSS22222222ooooWWWWyyyy2222LLLL++++3333SSSSzzzzSSSSbbbbssssbbbbooooQQQQSSSS++++iiii++++8888eeeeFFFFDDDDEEEEqqqq////////GGGGmmmm////////GGGGTTTTZZZZuuuuDDDDttttgggg4444ssssDDDDDDDDPPPPvvvvssssffffMMMMmmmmSSSSAAAATTTTXXXXxxxxnnnnWWWW////nnnnddddLLLLKKKK6666ttttrrrr6666RRRRnnnnmmmmzzzzssssrrrrWWWW9999ssss7777ttttXXXX3333TTTT9999oooo6666zzzzhhhhVVVVDDDDFFFFssssssssFFFFrrrrHHHHqqqqBBBBFFFFSSSSjjjj4444BBBBJJJJbbbbhhhhhhhhuuuuBBBBnnnnUUUUQQQQhhhhjjjjQQQQKKKKBBBBDDDD8888HHHH4444OOOOvvvvccccffffnnnnllllBBBBppppHHHHsssstttt7777MMMM0000nnnnQQQQjjjj++++hhhhQQQQ8888ppppAAAAzzzzaaaaqqqqzzzz00002222IIIIuuuuooooGGGGQQQQVVVVhhhhddddjjjjffffttttVVVV2222ttttuuuu3333ZZZZ2222BBBBLLLLBBBBOOOOvvvvIIIIDDDDUUUUoooo0000OOOOxxxxXXXXvvvv3333qqqqDDDDmmmmKKKKUUUURRRRSSSSssssMMMMEEEE1111bbbbrrrrrrrruuuuYYYYnnnnxxxxMMMM6666ooooMMMMZZZZwwwwKKKKnnnnllllVVVV6666qqqqMMMMaaaaFFFFssssTTTTIIIIffffYYYYttttVVVVTTTTSSSSCCCCLLLLWWWWffffzzzzRRRRJJJJPPPPyyyyYYYYllllVVVVBBBBiiiiSSSSMMMMllllXXXX3333SSSSkkkkJJJJnnnn6666eeeeyyyyOOOOjjjjkkkkddddaaaaTTTTKKKKLLLLCCCCTTTTeeeeUUUUKKKK99996666OOOOXXXXiiiiffff111144443333NNNNeeeeGGGGllllnnnn3333GGGGZZZZppppAAAAYYYYllllmmmm33338888UUUUppppooooKKKKYYYYmmmmOOOOTTTTnnnnkkkkwwwwFFFFXXXXyyyyIIIIyyyyYYYYWWWWEEEEKKKKZZZZ4444jjjjYYYYrrrrYYYYSSSSOOOOqqqqKKKKDDDDOOOO2222ppppIIIIoooottttwwwwVVVVssss8888eeeeZZZZmmmm0000zzzz++++qqqqeeeeWWWW////dddduuuuzzzz2222uuuuNNNNqqqq6666KKKKOOOOMMMMhhhhzzzzBBBBMMMMZZZZyyyyCCCCBBBBxxxxffffQQQQggggBBBBttttooooQQQQggggssssYYYYSSSSHHHHiiiiGGGGVVVV3333hhhhzzzzttttPPPPPPPPiiiivvvvDDDDssssffff88889999GGGGSSSSUUUU++++wwwwccccwwwwhhhh88884444nnnnzzzz////FFFFAAAAJJJJDDDD3333<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Speech Adaptor Speech Encoder LLaMA-Omni (Hey! Can you give me some advices on writing NLP papers?) ❄\"\" L<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"HHHHXXXXhhhhpppp2222GGGGGGGGFFFFtttt6666SSSSffffssssCCCC7777AAAAttttGGGGxxxxdddd8888eeeeTTTT7777iiii6666MMMM====\"\"\"\">>>>AAAAAAAAAAAABBBB////3333iiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqOOOODDDDGGGGTTTTbbbbAAAAIIIIrrrrkkkkooooiiiiggggiiii6666LLLLbbbbllllxxxxUUUUqqqqGGGGAAAAffff0000IIIIQQQQwwwwmmmmUUUU7777bbbbooooZZZZNNNNJJJJmmmmLLLLkkkkRRRRSSSS8888zzzzCCCCXXXX3333HHHHjjjjQQQQhhhhGGGG3333////ooooYYYY7777////8888ZZZZJJJJmmmm4444WWWW2222HHHHhhhhgggg4444nnnnHHHHMMMMvvvv99998888wwwwJJJJYYYYssss4444UUUU2222PPPPaaaa3333UUUUVVVVppppaaaaXXXXllllllllddddKKKK66669999XXXXNNNNjjjjaaaa3333ttttnnnnffffMMMM3333bbbb22222222iiiihhhhJJJJJJJJaaaaIIIIttttEEEEPPPPJJJJLLLLddddAAAACCCCvvvvKKKKmmmmaaaaAAAAttttYYYYMMMMBBBBppppNNNN5555YYYYUUUUhhhhwwwwGGGGnnnnnnnnWWWWBBBB8888llllffffuuuuddddeeeeyyyyooooVVVViiii8888QQQQddddTTTTGGGGLLLLqqqqhhhhXXXXggggoooo2222IIIIAAAARRRRDDDDFFFFrrrryyyyzzzzQQQQMMMM3333xxxxDDDDAAAAiiiimmmmKKKKeeeeNNNNzzzzHHHHeeeeBBBBPPPPkkkkDDDDaaaaaaaaNNNNxxxxkkkkvvvvllllmmmm1111aaaa////YYYYUUUU1111iiiiJJJJxxxxCCCCllllJJJJFFFFBBBBZZZZqqqq++++++++eeeeXXXX2222IIII5555KKKKEEEEVVVVAAAADDDDhhhhWWWWKKKKmmmmeeeeYYYY8888ffffggggppppVVVVggggCCCCIIII5555xxxxmmmmFFFFTTTTddddRRRRNNNNMMMMZZZZkkkkjjjjIIIIeeee0000pppp6666nnnnAAAAIIIIVVVVVVVVeeeeOOOOssss2222ffffWWWWccccddddaaaa6666VVVVuuuuDDDDSSSSOOOOoooonnnnwwwwJJJJqqqqqqqqvvvvzzzzddddSSSSHHHHCCCCoooo1111CCCCQQQQMMMM9999mmmmaaaaddddVVVV888811114444uuuu////uuuuffff1111EEEEhhhhhhhhcccceeeeCCCCkkkkTTTTccccQQQQJJJJUUUUkkkkNNNNmmmmhhhhQQQQccccIIIIttttiiiiKKKKyyyy8888DDDDKKKKvvvvPPPPJJJJCCCCXXXXAAAAJJJJ5555ppppggggIIIIppppnnnnOOOOaaaappppEEEERRRRllllppppiiiiAAAArrrrqqqqyyyyiiiiSSSS3333DDDDmmmmvvvv7777xxxxIIII2222qqqqcccc1111xxxx666644445555tttt2222ffffVVVV++++mmmmVVVVRRRRRRRRxxxxkkkkddddooooiiiiNNNN0000gggghhhhxxxx0000jjjjuuuurrrrooooGGGGjjjjVVVVRRRRCCCCxxxxHHHH0000iiiiJJJJ7777RRRRKKKK3333oooozzzznnnnoooowwwwXXXX4444999933334444mmmmIIII2222WWWWjjjjGGGGJJJJnnnnHHHH////2222BBBB8888ffffkkkkDDDDddddYYYYCCCCWWWWXXXXgggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> LLM Stage 1 Speech Decoder Upsample Large Language Model Speech Adaptor Speech Encoder ❄❄\"❄ L<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"CCCC4444RRRR////eeeezzzzuuuudddduuuuxxxx7777OOOOrrrrGGGGddddJJJJoooobbbbJJJJuuuuddddvvvvVVVV555522221111YYYY====\"\"\"\">>>>AAAAAAAAAAAABBBB////3333iiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqOOOODDDDGGGGTTTTbbbbAAAAIIIIrrrrkkkkooooiiiiggggiiii6666LLLL3333bbbbhhhhwwwwUUUUaaaaEEEEvvvvaaaaEEEEOOOOYYYYTTTTCCCCfffftttt0000MMMMkkkkkkkkzzzzNNNNyyyyIIIIJJJJWWWWbbbbhhhhrrrr7777hhhhxxxxooooYYYYhhhhbbbbffff8888OOOOddddffff++++OOOOkkkkzzzzUUUUJJJJbbbbDDDDwwwwwwwwcccczzzzrrrrmmmmXXXXeeee++++bbbb4444MMMMWWWWccccKKKKbbbbPPPPvvvvbbbbKKKKKKKK2222ssssrrrrqqqq1111vvvvllllDDDDccccrrrrWWWW9999ssss7777uuuu3333vvvvmmmm////kkkkFFFFHHHHRRRRYYYYkkkkkkkkttttEEEE0000iiiiHHHHssssmmmmeeeejjjjxxxxXXXXllllTTTTNNNNAAAA2222MMMMOOOOCCCC0000FFFF0000uuuuKKKKQQQQ5555////TTTTrrrrjjjj9999pppp5555HHHH77773333nnnnkkkkrrrrFFFFIIIIttttGGGGCCCCaaaaUUUUzzzzddddEEEEIIII8888EEEECCCCxxxxjjjjBBBBooooCCCCXXXXPPPPPPPPBBBBqqqqEEEEGGGGMMMMYYYYEEEE8888////QQQQ22228888wwwwZZZZAAAAHHHHyyyyBBBBttttttttBBBBqqqqZZZZZZZZ1111bbbbttttmmmmjjjj2222DDDDttttUUUUyyyyccccggggllllRRRRRRRRggggaaaaZZZZnnnnffffgggg2222GGGGEEEEUUUUllllCCCCKKKKooooBBBBwwwwrrrrFFFFTTTTffffssssWWWWNNNNwwwwUUUUyyyyyyyyBBBBEEEEUUUU6666zzzzyyyyiiiiBBBBRRRRNNNNMMMMZZZZkkkkggggkkkkeeee0000rrrr6666nnnnAAAAIIIIVVVVVVVVuuuuOOOOssssuuuuffffWWWWaaaaddddaaaaGGGGVVVVppppBBBBJJJJPPPPUUUUTTTTYYYYMMMM3333UUUU3333xxxxssssppppDDDDppppWWWWaaaahhhhrrrr6666eeeezzzzNNNNOOOOqqqqRRRRSSSS8888XXXX////////PPPP6666CCCCQQQQRRRRXXXXbbbbssssppppEEEEnnnnAAAAAAAAVVVVZZZZHHHH4444ooooSSSSLLLLggggFFFFkkkkZZZZWWWWXXXXYYYYQQQQ2222ZZZZppppAAAATTTT4444VVVVBBBBNNNNMMMMJJJJNNNNNNNNZZZZLLLLTTTTLLLLGGGGEEEEhhhhPPPPQQQQllllVVVVVVVV0000CCCCcccc7777iiiillll5555ddddJJJJ55557777zzzzmmmm2222DDDDXXXXnnnn7777qqqqJJJJaaaavvvvyyyy7777qqqqKKKKKKKKNNNNjjjjddddIIIILLLLOOOOkkkkIIIIMMMMuuuuUUUURRRR3333ddddooooCCCCZZZZqqqqIIII4444IIIIeeee0000TTTTNNNN6666RRRRWWWW////GGGGkkkk////FFFFiiiivvvvBBBBssssffff88889999GGGGSSSSUUUUeeeewwwwccccoooojjjj8888wwwwPPPPnnnn8888AAAAZZZZLLLL++++WWWWUUUUwwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> CTC Stage 2 Figure2: Left: ModelarchitectureofLLaMA-Omni. Right: Illustrationofthetwo-stagetraining strategyforLLaMA-Omni. Next, H′ is passed through a 2-layer perceptron with ReLU activation between the linear layers, resultinginthefinalspeechrepresentationS. Theaboveprocesscanbeformalizedasfollows: S= (H)=Linear(ReLU(Linear(DownSample(H)))). (2) A 2.3 LARGELANGUAGEMODEL We use Llama-3.1-8B-Instruct3 (Dubey et al., 2024) as the LLM , which is currently the state- M of-the-art open-source LLM. It has strong reasoning capabilities and is well-aligned with human preferences. Theprompttemplate ()isshowninFigure3. ThespeechrepresentationsequenceS isfilledintothepositioncorresponP din· gto<speech>,andthentheentiresequence (S)isinput into the LLM. Finally, the LLM autoregressively generates the text response YT =P [yT,...,yT ] 1 M directlybasedonthespeechinstructionandistrainedusingcross-entropyloss: M = logP(yT (S),YT). (3) LLLM − i |P <i i=1 (cid:88) 2.4 SPEECHDECODER ForthespeechresponseYS,wefirstfollow Zhangetal.(2023)todiscretizethespeechintodiscrete units. Specifically, we use the pretrained HuBERT (Hsu et al., 2021) model to extract continuous representations of the speech, and then convert these representations into discrete cluster indices usingaK-meansmodel. Subsequently,consecutiveidenticalindicesaremergedintoasingleunit, resultinginthefinaldiscreteunitsequenceYU =[yU,...,yU],yU 0,1,...,K 1 , 1 i L, 1 L i ∈{ − } ∀ ≤ ≤ whereK isthenumberofclusters,andListhelengthofdiscreteunitsequence. Thediscreteunits canbeconvertedintowaveformwithanadditionalunit-basedvocoder (Polyaketal.,2021). V To generate speech responses simultaneously with text responses, we add a streaming speech de- coder after the LLM. It consists of several standard Transformer (Vaswani et al., 2017) layers D withthesamearchitectureasLLaMA(Dubeyetal.,2024),eachcontainingacausalself-attention moduleandafeed-forwardnetwork. SimilartoMaetal.(2024a);Zhangetal.(2024b),thespeech decoderrunsinanon-autoregressivemanner,whichtakestheoutputhiddenstatesfromtheLLMas input,andgeneratesthediscreteunitsequencecorrespondingtothespeechresponse. Specifically, theoutputhiddenstatescorrespondingtothetextresponsearedenotedasC = [c ,...,c ],where 1 M c = ( (S),YT). Wefirstupsampleeachhiddenstateintoachunkbyafactorofλ,resultingin i M P <i 3https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct 3 Preprint <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful language and speech assistant. You are able to understand the speech content that the user provides, and assist the user with a variety of tasks using natural language.<|eot_id|> <|start_header_id|>user<|end_header_id|> <speech> Please answer the questions in the user’s input speech.<|eot_id|> <|start_header_id|>assistant<|end_header_id|> Figure3: PrompttemplateofLLaMA-Omni. anupsampledhiddenstatesequenceC = [c ,...c ],wherec = c . Next,Cisfedintothe 1 λ·M i ⌊i/λ⌋ speechdecoder ,andtheoutputhiddenstatesequenceisdenotedasO = [o ,...,o ]. Weuse 1 λ·M D connectionist temporal classification(cid:98)(CTC; (cid:98)Grav (cid:98)es et al., 2006 (cid:98)a) to align O with(cid:98)the discrete unit sequenceYU. Specifically,CTCextendstheoutputspacewithaspecialblanktokenϵ: P(a O)=softmax(Wo +b)[a ], a 0,1,...,K 1,ϵ , (4) i i i i | ∀ ∈{ − } whereW R(K+1)×d andb RK+1 areweightsandbiasesofthelinearlayer,andthesequence ∈ ∈ A=[a ,...,a ]isknownasthealignment. Tomodelthevariable-lengthmappingbetweeninput 1 λ·M andoutput,CTCintroducesacollapsingfunctionβ(A),whichfirstmergesallconsecutiverepeated tokensinAandtheneliminatesallblanktokensϵ. Forinstance: β([1,1,2,ϵ,ϵ,2,3]) = [1,2,2,3]. Duringtraining,CTCperformsmarginalizationoverallpossiblealignmentsasfollows: λ·M = logP(YU O)= log P(AO)= log P(a O), (5) CTC i L − | − | − | A∈β (cid:88)−1(YU) A∈β (cid:88)−1(YU)i (cid:89)=1 whereβ−1(YU)denotesallpossiblealignmentsoflengthλ M thatcanbecollapsedtoYU. The · alignmentismodeledinanon-autoregressiveway. Duringinference, weselectthebestalignment A∗ = argmax P(AO), and apply the collapsing function to obtain the discrete unit sequence A β(A∗),whichisthenfe| dintothevocodertosynthesizewaveform. 2.5 TRAINING AsshowninFigure2,weadoptatwo-stagetrainingstrategyforLLaMA-Omni. Inthefirststage, wetrainthemodeltogeneratetextresponsesdirectlyfromthespeechinstructions. Specifically,the speechencoderisfrozen,andthespeechadaptorandtheLLMaretrainedusingtheobjective LLM L inEq. (3). Thespeechdecoderisnotinvolvedintrainingduringthisstage. Inthesecondstage,we trainthemodeltogeneratespeechresponses. Duringthisstage,thespeechencoder,speechadaptor, andLLMareallfrozen,andonlythespeechdecoderistrainedusingtheobjective inEq. (5). CTC L 2.6 INFERENCE Duringinference,theLLMautoregressivelygeneratesthetextresponsebasedonthespeechinstruc- tion. Meanwhile, since our speech decoder uses causal attention, once the LLM generates a text response prefix YT, the corresponding upsampled hidden states C can be fed into the speech ≤i ≤λ·i decodertogenerateapartialalignmentA ,whichinturnyieldsthediscreteunitscorresponding ≤λ·i tothegeneratedtextprefix. Tofurtherenablestreamingsynthesis(cid:98)ofspeechwaveforms,whenthe numberofgeneratedunitsreachesapre-definedchunksizeΩ, weinputthisunitsegmentintothe vocodertosynthesizeaspeechsegment,whichisthenimmediatelyplayedtotheuser. Asaresult, userscanstartlisteningtothespeechresponsewithoutwaitingforthecompletetextresponsetobe generated,ensuringlowresponselatencythatisnotaffectedbythelengthofthetextresponse.Algo- rithm1describestheaboveprocess. Additionally,sincethespeechdecoderusesnon-autoregressive modeling,thealignmentcorrespondingtoeachtexttokenyT,specificallyA ,isgener- i λ·(i−1)+1:λ·i atedinparallelwithinthechunk. Therefore,thedecodingspeedforgeneratingbothtextandspeech simultaneouslyisnotsignificantlydifferentfromthespeedofgeneratingtextalone. 4 Preprint \n",
      "\n",
      "3 CONSTRUCTION OF SPEECH INSTRUCTION DATA: INSTRUCTS2S-200K: To train LLaMA-Omni, we need triplet data consist- ing of <speech instruction, text response, speech re- Algorithm1:InferenceProcess sponse>. However, most publicly available instruc- Input:speechinstructionXS. tion data is in text form. Therefore, we construct Output:textoutputsYT,unitsoutputs speechinstructiondatabasedonexistingtextinstruc- YU,waveformoutputsYS. tiondatathroughthefollowingprocess: Model:speechencoderE,speechadaptor A,LLMM,speechdecoderD, vocoderV. Step 1: Instruction Rewriting Since speech input Require:MinimumchunksizeforunitsΩ. hasdifferentcharacteristicscomparedtotextinput,we Initialization:i=1,j =0,YT =[], rewritethetextinstructionsaccordingtothefollowing YU =[],YS =[],C(cid:98) =[]. rules: (1)Addappropriatefillerwords(suchas“hey”, whiley iT −1 ̸=⟨EOS⟩do “so”,“uh”,“um”,etc.) totheinstructionstosimulate S←A(E(XS)); naturalspeechpatterns. (2)Convertnon-textsymbols c i ←M(P(S),Y <T i); in the instructions (such as numbers) into their corre- y iT ←argmax yT P(y iT|P(S),Y <T i); i spondingspokenformstoensurecorrectsynthesisby YT ←YT +yT; i TTS.(3)Modifytheinstructionstoberelativelybrief C(cid:98) ←C(cid:98) +UpSample(c i); withoutexcessiveverbiage. WeusetheLlama-3-70B- O←D(C(cid:98)); Instruct4modeltorewritetheinstructionsaccordingto A∗ ←argmax P(A|O); A theserules. ThepromptcanbefoundinAppendixA. YU ←β(A∗); if |YU|−j ≥Ωthen yS ←V(YU ); Step 2: Response Generation In speech interac- j+1: YS ←YS+yS; tions,existingresponsesfromtextinstructionsarenot j ←|YU|; suitablefordirectuseasspeechinstructionresponses. end This is because, in text-based interactions, models i←i+1; tendtogeneratelengthyresponses,usingcomplexsen- end tencesandpossiblyincludingnon-verbalelementslike if j <|YU|then ordered lists or parentheses. However, in speech in- yS ←V(YU ); j+1: teractions, concise yet informative responses are typ- YS ←YS+yS; ically preferred (Anonymous, 2024). Therefore, we end use the Llama-3-70B-Instruct model to generate re- sponses for speech instructions according to the fol- lowing rules: (1) The response should not contain content that cannot be synthesized by the TTS model, such as parentheses, ordered lists, etc. (2) The response should be very concise and to the point,avoidinglengthyexplanations. ThepromptcanbefoundinAppendixA. Step 3: Speech Synthesis After obtaining the instructions and responses suitable for speech in- teractions,weneedtofurtherconvertthemintospeechusingTTSmodels. Fortheinstructions,to makethesynthesizedspeechsoundmorenatural,weusetheCosyVoice-300M-SFT(Duetal.,2024) model5, randomly selecting either a male or female voice for each instruction. For the responses, weusetheVITS(Kimetal.,2021)model6 trainedontheLJSpeech(Ito&Johnson,2017)dataset tosynthesizetheresponsesintoastandardvoice. Forthebasictextinstructions, wecollectaround50KinstructionsfromtheAlpacadataset7 (Taori etal.,2023),whichcoversawiderangeoftopics. Additionally,wegatheraround150Kinstructions fromtheUltraChatdataset8(Dingetal.,2023),whichprimarilyconsistofquestionsabouttheworld. Note that UltraChat is a large-scale multi-turn conversation dataset, but we only select the first 150Kentriesanduseonlythefirst-roundinstruction. Usingtheabovedatasetsanddataprocessing pipeline,weultimatelyobtain200Kspeechinstructiondata,referredtoasInstructS2S-200K. 4https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct 5https://github.com/FunAudioLLM/CosyVoice 6https://github.com/jaywalnut310/vits 7https://huggingface.co/datasets/tatsu-lab/alpaca 8https://github.com/thunlp/UltraChat 5 Preprint \n",
      "\n",
      "4 EXPERIMENTS: 4.1 EXPERIMENTALSETUPS Datasets For the training data, we use the InstructS2S-200K dataset mentioned in Section 3, whichincludes200Kspeechinstructiondata. Toextractdiscreteunitscorrespondingtothetarget speech,weuseapre-trainedK-meansquantizer9,whichhaslearned1000clustersfromtheHuBERT features. The pretrained HiFi-GAN vocoder (Kong et al., 2020; Polyak et al., 2021) is used to synthesizediscreteunitsintowaveform.Fortheevaluationdata,weselecttwosubsetsfromAlpaca- Eval10 (Li et al., 2023): helpful base and vicuna, as their questions are more suitable for speech interaction scenarios. We remove questions related to math and code, resulting in a total of 199 instructions. To obtain the speech version, we use the CosyVoice-300M-SFT model to synthesize theinstructionsintospeech. WerefertothistestsetasInstructS2S-Evalinthefollowingsections. Model Configuration We use the encoder of Whisper-large-v3 as the speech encoder, and use Llama-3.1-8B-InstructastheLLM.Thespeechadapterperformsa5 downsamplingonthespeech × representations. The speech decoder consists of 2 Transformer layers with the same architecture as LLaMA, with a hidden dimension of 4096, 32 attention heads, and a feed-forward network di- mension of 11008, which contains 425M parameters. The upsample factor λ is set to 25. For the minimumunitchunksizeΩinputtothevocoder,wesetΩ=+ inthemainexperiment,meaning ∞ we wait for the entire unit sequence to be generated before inputting it to the vocoder for speech synthesis. In subsequent experiments, we will analyze how adjusting the value of Ω can control responselatency,aswellasthetrade-offbetweenlatencyandspeechquality. Training LLaMA-Omnifollowsatwo-stagetrainingprocess.Inthefirststage,wetrainthespeech adapterandtheLLMwithabatchsizeof32for3epochs. Weuseacosinelearningratescheduler with the first 3% of steps for warmup, and the peak learning rate is set to 2e-5. In the second stage, we train the speech decoder, using the same batch size, number of steps, and learning rate schedulerasthefirststage, butwiththepeaklearningratesetto2e-4. Theentiretrainingprocess takesapproximately65hourson4NVIDIAL40GPUs. 4.2 EVALUATION Since LLaMA-Omni can generate both text and speech responses based on speech instructions, we evaluate the model’s performance on two tasks: speech-to-text instruction-following (S2TIF) and speech-to-speech instruction-following (S2SIF). We use greedy search to ensure reproducible experimentalresults. Themodelisevaluatedfromthefollowingaspects: ChatGPT Score To evaluate the model’s ability to follow speech instructions, we use GPT- 4o (OpenAI, 2024) to score the model’s responses. For the S2TIF task, scoring is based on the transcribedtextofthespeechinstructionsandthemodel’stextresponse.FortheS2SIFtask,wefirst transcribethemodel’sspeechresponsesintotextusingtheWhisper-large-v3model,andthenscore itinthesamemannerastheS2TIFtask. GPT-4ogivesscoresontwoaspects:contentandstyle.The content score evaluates whether the model’s response adequately addresses the user’s instruction, whilethestylescoreassesseswhetherthemodel’sresponsestyleissuitableforspeechinteraction scenarios. ThedetailedpromptcanbefoundinAppendixA. Speech-TextAlignment Toevaluatethealignmentbetweentextresponsesandspeechresponses, weusetheWhisper-large-v3modeltotranscribethespeechresponsesintotext,andthencalculate theWordErrorRate(WER)andCharacterErrorRate(CER)betweenthetranscribedtextandthe textresponse. WerefertothesemetricsasASR-WERandASR-CER,respectively. 9https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3_ L11_km1000.bin 10https://github.com/tatsu-lab/alpaca_eval 6 Preprint Table1:ChatGPTscoresforS2TIFandS2SIFtasksontheInstructS2S-Evalbenchmark,alongwith thealignmentscoresbetweenspeechandtextresponses. HerewesetΩ=+ fortheS2SIFtask. ∞ S2TIF S2SIF Alignment Model Content Style Content Style ASR-WER ASR-CER ↑ ↑ ↑ ↑ ↓ ↓ SpeechGPT 2.59 3.15 1.58 1.81 47.62 37.26 SALMONN(+TTS) 2.57 2.79 2.46 2.84 21.77 20.25 Qwen2-Audio(+TTS) 2.73 2.64 2.32 2.58 55.72 53.09 LLaMA-Omni 3.23 3.81 2.69 3.12 11.61 7.59 SpeechQuality Toevaluatethequalityofthegeneratedspeech,weutilizeaMeanOpinionScore (MOS) prediction model called UTMOS11 (Saeki et al., 2022), which is capable of predicting the MOSscoreofthespeechtoassessitsnaturalness. WerefertothismetricastheUTMOSscore. ResponseLatency Thelatencyisakeymetricforspeechinteractionmodels,referringtothetime interval between the input of a speech instruction and the start of the speech response, which has a significant impact on user experience. Additionally, we calculate the number of words already generatedinthetextresponsewhenthespeechresponsebegins,referredtoasthe#laggingword. 4.3 BASELINESYSTEMS Weincludethefollowingspeech-languagemodelsasbaselinesystems: SpeechGPT SpeechGPT (Zhang et al., 2023) is a speech-language model that supports both speech input and output. We use the chain-of-modality prompting adopted in the original paper for decoding, which sequentially outputs the text instruction, text response, and speech response basedonthespeechinstruction. SALMONN(+TTS) SALMONN(Tangetal.,2024)isaLLMcapableofacceptingspeechand audio inputs and responding with text, enabling it to perform the S2TIF task. For the S2SIF task, weaddaVITSTTSmodelafterSALMOONtogeneratespeechresponsesinacascadedmanner. Qwen2-Audio(+TTS) Qwen2-Audio(Chuetal.,2024)isapowerfulgeneral-purposeaudioun- derstandingmodelcapableofperformingvariousaudio-relatedtasks,includingtheS2TIFtask. We alsobuildacascadedsystemwithQwen2-AudioandVITStocompletetheS2SIFtask. 4.4 MAINRESULTS Table 1 presents the main results on the InstructS2S-Eval benchmark. First, for the S2TIF task, fromthecontentperspective,LLaMA-Omnishowssignificantimprovementcomparedtoprevious models. This is mainly because LLaMA-Omni is developed based on the latest Llama-3.1-8B- Instructmodel,leveragingitsstrongtextinstruction-followingcapabilities. Fromthestyleperspec- tive,SALMONNandQwen2-Audioreceivelowerscores,astheyarespeech-to-textmodels. Their outputstyleisnotalignedwithspeechinteractionscenarios,oftenproducingformattedcontentand containingalot ofredundantexplanations. Incontrast, SpeechGPT,asa speech-to-speech model, achievesahigherstylescore. Similarly,ourLLaMA-Omniattainsthehigheststylescore,indicating thatafterbeingtrainedonourInstructS2S-200Kdataset,theoutputstylehasbeenwell-alignedwith speechinteractionscenarios. FortheS2SIFtask,LLaMA-Omnialsooutperformspreviousmodels inbothcontentandstylescores. ThisfurtherconfirmsthatLLaMA-Omniiscapableofeffectively addressinguser’sinstructionswithspeechinaconciseandefficientmanner. Additionally, in terms of alignment between speech and text responses, LLaMA-Omni achieves thelowestASR-WERandASR-CERscores. Incontrast, SpeechGPTperformspoorlyinaligning speech and text responses, likely due to its sequential generation of text and speech. The speech- text alignment of cascaded systems, such as SALMONN+TTS and Qwen2-Audio+TTS, is also 11https://github.com/tarepan/SpeechMOS 7 Preprint Table2: Latency,speech-textalignmentandspeechqualityunderdifferentunitchunksizes. ChunkSizeΩ Latency(ms) #LaggingWord ASR-WER ASR-CER UTMOS ↓ ↓ ↑ 10 226.13 1.82 10.44 6.94 3.2304 20 256.28 2.83 10.51 6.98 3.4748 40 326.63 4.68 10.99 7.24 3.6688 60 391.96 6.47 11.17 7.27 3.7549 80 467.34 8.29 11.40 7.45 3.7858 100 527.64 10.02 11.48 7.45 3.8242 + 1924.62 41.40 11.61 7.59 3.9296 ∞ suboptimal, primarily because the generated text responses may contain characters that cannot be synthesizedintospeech. ThisissueisespeciallyevidentinQwen2-Audio,whichoccasionallyout- putsChinesecharacters,introducingerrorsinthespeechresponses. Incomparison,LLaMA-Omni achievesthelowestASR-WERandASR-CERscores,demonstratingahigherdegreeofalignment betweengeneratedspeechandtextresponses,andfurthervalidatingtheadvantageofourapproach insimultaneouslygeneratingbothtextandspeechresponses. 4.5 TRADE-OFFBETWEENSPEECHQUALITYANDRESPONSELATENCY LLaMA-Omnicansimultaneouslygeneratebothtextresponsesanddiscreteunitscorrespondingto thespeechresponse. AsdescribedinSection2.6,tofurtherenablestreamingwaveformgeneration, when the number of generated discrete units reaches a certain chunk size Ω, the unit chunk is fed into the vocoder to synthesize and play the speech. By adjusting the value of Ω, we can control the system’s latency, where a smaller Ω corresponds to lower system latency. When Ω = + , it ∞ equates to waiting for all units to be generated before synthesizing the speech. At the same time, thevalueofΩalsoaffectsthequalityofthegeneratedspeech. AsmallerΩmeansthatthespeechis dividedintomoresegmentsforsynthesis,whichmayresultindiscontinuitiesbetweenthesegments, potentiallyreducingtheoverallcoherenceofthespeech. TobetterunderstandtheimpactofΩ,weexplorethesystem’slatency,thealignmentbetweenspeech andtextresponses,andthequalityofthegeneratedspeechunderdifferentΩsettings. Asshownin Table 2, when Ω is set to 10, the system’s response latency is as low as 226ms, which is even lower than GPT-4o’s average audio latency of 320ms. At this point, the speech response lags by an average of 1.82 words at the start. When Ω is set to + , the latency increases to around 2 ∞ seconds. FortheASR-WERandASR-CERmetrics,wearesurprisedtofindthatasthechunksize increases,theerrorratesalsoincrease. Webelievetheremaybetworeasonsforthis. Ononehand, thevocodermayhandleshortunitsequencesmorereliablythanlongones,asitistypicallytrained on shorter sequences. On the other hand, the ASR model we use, Whisper-large-v3, has strong robustness. EvenwhenthespeechissomewhatdiscontinuouswithsmallerΩ,ithaslittleimpacton ASRrecognitionaccuracy. Therefore, wefurtherevaluatethenaturalnessofthegeneratedspeech usingtheUTMOSmetric.ItshowsthatasΩincreases,thenaturalnessofthespeechimproves,since discontinuitiesinthespeechdecrease. Insummary,wecanadjustthevalueofΩbasedondifferent scenariostoachieveatrade-offbetweenresponselatencyandspeechquality. 4.6 DECODINGTIME Table 3: Average decoding time(s)ofdifferentmodelson Table3liststheaveragedecodingtimesofdifferentmodelsonthe S2TIFandS2SIFtasks. S2TIF and S2SIF tasks. For the S2TIF task, SpeechGPT needs to first output the text instruction and then the text response, while Model S2TIF S2SIF SALMONNandQwen2-Audiotendtoproducelengthyresponses. SpeechGPT 4.28 25.60 In contrast, LLaMA-Omni provides concise answers directly, re- SALMONN 4.78 / Qwen2-Audio 8.42 / sulting in significantly lower decoding times, with an average of LLaMA-Omni 1.49 1.92 only1.49secondsperinstruction. FortheS2SIFtask,SpeechGPT serially outputs the text and speech responses, resulting in a de- coding time approximately 6 times longer than when generating only text responses. In con- trast, LLaMA-Omni outputs both text and speech responses simultaneously and employs a non- 8 Preprint Table4: Responsesfromdifferentmodelsfortheinstruction: “HowdoIwrapapresentneatly?” Model Response Qwen2-Audio Wrappingapresentneatlycanbeabittricky,butwithsomepracticeandpatience,youcanmastertheart ofwrappingpresentsbeautifully. Herearesomestepstofollow: n n1. Startwithaclean,flatsurface: Findaclean,flatsurfacewhereyoucanworkonwrappingyourpre\\se\\nt.Alargetableorcountertopworks well. n n2. Gatheryoursupplies:Youwillneedwrappingpaper,giftwrapscissors,arulerormeasuring tape,\\an\\danyotherdecorationsyouwanttouse(suchasribbon,bows,orstickers). n n3.Measurethegift: Usetherulerormeasuringtapetomeasurethewidthandlengthofthegift.Make\\su\\retoaddalittleextra spacearoundtheedgesforwrapping. n n4. Cutthewrappingpaper: Cutthewrappingpapertothesize youmeasured. Ifyouhavemultiplep\\ie\\cesofwrappingpaper,layerthemtogethersothattheyfitneatly aroundthegift. n n5.Foldthepaper:Beginbyfoldingoneofthesidesofthewrappingpaperoverthegift, creaseitwell,a\\nd\\thenfoldtheothersideoverthetop.Thiscreatesasimple,yetattractivefold. n n6.Add decorations:Ifyoulike,youcanadddecorationssuchasribbon,bows,orstickerstothegift.Si\\mp\\lyattach themtothetopofthegiftusingglueordouble-sidedtape. n n7. Wrapthegift: Startingatthebottom ofthegift,placeapieceofwrappingpaperoverit. Bringth\\e\\leftsideofthepaperupoverthegift,tuck itintightly,andpressitdown. Repeatthisprocessontherightsideofthegift,bringingtherightsideof thepaperupoverthegift,tuckingitintightly,andpressingitdown. n n8. Finishtheedges: Onceyou havewrappedthegift,trimoffanyexcesspaperalongtheedgesusing\\th\\egiftwrapscissors.Youcanalso addadditionaltouchessuchasabowordecorativelabeltocompletethelook. n nRemember,thekeyto wrappingapresentneatlyistotakeyourtimeandpayattentiontodetail. With\\p\\ractice,you’llbeableto createbeautifulwrappingpaperpresentationseverytime! SALMOON Towrapapresentneatly,youcanstartbylayingoutacleanandflatsurfacetoworkon. Then,placethe presentonthesurfaceandwrapitinapieceofwrappingpaperthatisslightlylargerthanthepresent.Fold thepaperinhalfandtheninhalfagain,sothatthetopandbottomflapsmeetinthemiddle.Then,foldthe leftandrightflapsintowardsthecenter,sothattheymeetatthetopandbottomofthepresent.Finally,seal thepresentbypressingtheflapstogetherandusingsometapetosecurethem. SpeechGPT Wrappingapresentneatlyrequiresafewsimplesteps. First,youshouldfoldthepaperinhalfandthen unfoldit.Then,youshouldplacethepresentinthecenterofthepaperandfoldthepaperaroundit.Finally, youshouldtietheendsofthepaperwitharibbonorstring. LLaMA-Omni Towrapapresentneatly,startwithaflatsurface,placethegiftinthecenter,andfoldoneedgeofthe wrappingpaperoverthegift,creaseit,andtapeitdown. Repeatthisprocesswiththeremainingedges, makingsuretoalignthemevenlyandsmoothoutanyairbubbles. autoregressivearchitectureforgeneratingdiscreteunits. Asaresult,thetotalgenerationtimeonly increasesby1.28times,demonstratingLLaMA-Omni’sadvantageindecodingspeed. 4.7 CASESTUDY Tointuitivelyunderstandthedifferencesinresponsesfromdifferentmodels,weprovideanexample in Table 4. It can be observed that the response of Qwen2-Audio are quite lengthy and include elementssuchaslinebreaksandparenthesesthatcannotbesynthesizedintospeech. Theresponse from SALMONN is also a bit long. The style of SpeechGPT’s response is more appropriate for speech interaction scenarios, but the amount of information contained in its responses is less. In contrast, the response given by LLaMA-Omni is more detailed and helpful while maintaining a concisestyle,outperformingpreviousmodelsinspeechinteractionscenarios. \n",
      "\n",
      "5 RELATED WORK: Speech/AudioLanguageModels Withthesuccessoflanguagemodelsinthefieldofnaturallan- guage processing (Brown et al., 2020), researchers have begun exploring how to model speech or audiousinglanguagemodels. Earlyworkattemptedtotrainlanguagemodelsonsemantictokensor acoustictokensofaudio,enablingthegenerationofaudiowithouttheneedfortext(Lakhotiaetal., 2021; Nguyen et al., 2023; Borsos et al., 2023). Furthermore, by jointly training speech tokens andtext, decoder-onlymodelslikeVALL-E(Wangetal.,2023b)andVioLA(Wangetal.,2023c) can perform tasks such as speech recognition, speech translation, and speech synthesis. However, the above models are not built upon LLMs. To harness the power of LLMs, many studies ex- plore how to build speech-language models based on LLMs like LLaMA, which can be further dividedintotwotypes. Thefirsttype, representedbySpeechGPT(Zhangetal.,2023;2024a)and AudioPaLM (Rubenstein et al., 2023), involves creating native multimodal speech-text models by adding speech tokens to the LLM’s vocabulary and continuing pretraining using speech and text data. However, this approach typically requires a large amount of data and substantial computa- 9 Preprint tionalresources. ThesecondtypetypicallyinvolvesaddingaspeechencoderbeforetheLLMand finetuning the entire model to equip it with speech understanding capabilities (Shu et al., 2023; Deshmukh et al., 2023), such as speech recognition (Fathullah et al., 2024a; Yu et al., 2024; Ma etal.,2024c;Honoetal.,2024),speechtranslation(Wuetal.,2023;Wangetal.,2023a;Chenetal., 2024), or other general speech-to-text tasks (Chu et al., 2023; Tang et al., 2024; Chu et al., 2024; Fathullahetal.,2024b;Dasetal.,2024;Huetal.,2024). However,theseapproachestypicallyfocus onlyonspeechoraudiounderstandingwithouttheabilitytogeneratethem. Comparedtoprevious work,LLaMA-OmniequipstheLLMwithbothspeechunderstandingandgenerationcapabilities, enablingittoperformgeneralspeechinstruction-followingtasks. Additionally,LLaMA-Omnihas alowtrainingcost,makingitconvenientfordevelopmentbasedonthelatestLLMs. Simultaneous Generation Streaming generation aims to begin producing output before the en- tire input is received. This capability is crucial for maintaining synchronization between speakers andlistenersinvariousscenarios,suchasstreamingspeechrecognitionandsimultaneousinterpre- tation. In the case of large language models, having a streaming speech synthesis component can significantly reduce latency between the model and its users. Popular streaming generation meth- odsfallintothreemaincategories: monotonic-attention-basedmethods(Raffeletal.,2017),CTC- basedmethods(Gravesetal.,2006b),andTransducer-basedmethods(Graves,2012). Monotonic- attention-based methods modify the traditional attention-based sequence-to-sequence framework (Bahdanau, 2014) to support streaming generation. These methods rely on an external module to manage the READ/WRITE policy, which can be either fixed (e.g., Wait-k (Ma et al., 2018)) or adaptive (e.g., MMA (Ma et al., 2019), EDAtt (Papi et al., 2022), Seg2Seg (Zhang & Feng, 2024)). CTC-basedmethodsaddablanksymboltothetargetvocabularytorepresentaWAITac- tion. Streaminginferenceisachievedbyremovingadjacentrepetitivetokensandblanksymbols. To leveragethestrengthsofattention-basedmethods,CTC-basedapproachesoftenuseachunk-based non-autoregressivearchitecture(Maetal.,2023),whichhasproveneffectiveinsimultaneousinter- pretationandstreamingspeechsynthesis(Zhangetal.,2024b;Maetal.,2024a). Transducer-based methodsaredesignedtobridgethegapbetweenthenon-autoregressivenatureofCTC-basedmeth- ods and the autoregressive dependency between target tokens. These approaches introduce an ad- ditionalpredictortocapturetokendependencies,andtheirvariantshaveshownstrongperformance insimultaneousinterpretation(Liuetal.,2021;Tangetal.,2023). Morerecently,researchershave begun adopting decoder-only large language models for streaming generation tasks (Seide et al., 2024;Guoetal.,2024),andextendingthemtoaninterruptibleduplexmodel(Maetal.,2024b). 6 CONCLUSION In this paper, we propose an innovative model architecture, LLaMA-Omni, which enables low- latency and high-quality speech interaction with LLMs. LLaMA-Omni is built upon the latest Llama-3.1-8B-Instruct model, with the addition of a speech encoder for speech understanding and a streaming speech decoder that can generate both text and speech responses simultaneously. To align the model with speech interaction scenarios, we construct a speech instruction dataset InstructionS2S-200K, which contains 200K speech instructions along with the speech responses. Experimentalresultsshowthat,comparedtopreviousspeech-languagemodels,LLaMA-Omnide- liverssuperiorresponsesinbothcontentandstyle,witharesponselatencyaslowas226ms. More- over, training LLaMA-Omni requires less than 3 days on 4 GPUs, enabling rapid development of speechinteractionmodelsbasedonthelatestLLMs. Inthefuture,weplantoexploreenhancingthe expressivenessofgeneratedspeechresponsesandimprovingreal-timeinteractioncapabilities. REFERENCES Anonymous. Speechworthyinstruction-tunedlanguagemodels. https://openreview.net/ forum?id=SsAHMcacaQ,2024. ACLARR2024FebruaryBlindSubmission. Dzmitry Bahdanau. Neural machine translation by jointly learning to align and translate. arXiv preprintarXiv:1409.0473,2014. Zala´nBorsos,Raphae¨lMarinier,DamienVincent,EugeneKharitonov,OlivierPietquin,MattShar- ifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a 10 Preprint languagemodelingapproachtoaudiogeneration. IEEE/ACMtransactionsonaudio,speech,and languageprocessing,31:2523–2533,2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu- ral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. XiChen,SongyangZhang,QibingBai,KaiChen,andSatoshiNakamura. LLaST:Improvedend- to-end speech translation system leveraged by large language models. In Lun-Wei Ku, An- dre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Lin- guistics ACL 2024, pp. 6976–6987, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-acl.416. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-languagemodels. arXivpreprintarXiv:2311.07919,2023. YunfeiChu,JinXu,QianYang,HaojieWei,XipinWei,ZhifangGuo,YichongLeng,YuanjunLv, JinzhengHe,JunyangLin,ChangZhou,andJingrenZhou. Qwen2-audiotechnicalreport. arXiv preprintarXiv:2407.10759,2024. NilakshDas,SaketDingliwal,SrikanthRonanki,RohitPaturi,DavidHuang,PrashantMathur,Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, et al. Speechverse: A large-scale generalizableaudiolanguagemodel. arXivpreprintarXiv:2405.08295,2024. SohamDeshmukh,BenjaminElizalde,RitaSingh,andHuamingWang. Pengi: Anaudiolanguage model for audio tasks. Advances in Neural Information Processing Systems, 36:18090–18108, 2023. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXivpreprintarXiv:2305.14233,2023. ZhihaoDu,QianChen,ShiliangZhang,KaiHu,HengLu,YexinYang,HangruiHu,SiqiZheng,Yue Gu, Ziyang Ma, et al. Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer basedonsupervisedsemantictokens. arXivpreprintarXiv:2407.05407,2024. AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels. arXivpreprintarXiv:2407.21783,2024. YassirFathullah,ChunyangWu,EgorLakomkin,JuntengJia,YuanShangguan,KeLi,JinxiGuo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. Prompting large language models with speechrecognitionabilities. InICASSP2024-2024IEEEInternationalConferenceonAcoustics, SpeechandSignalProcessing(ICASSP),pp.13351–13355.IEEE,2024a. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Ke Li, Junteng Jia, Yuan Shangguan, Jay Ma- hadeokar,OzlemKalinli,ChristianFuegen,andMikeSeltzer. Audiochatllama:Towardsgeneral- purposespeechabilitiesforllms. InProceedingsofthe2024ConferenceoftheNorthAmerican ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies(Vol- ume1: LongPapers),pp.5522–5532,2024b. Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711,2012. 11 Preprint AlexGraves,SantiagoFerna´ndez,FaustinoGomez,andJu¨rgenSchmidhuber.Connectionisttempo- ralclassification: Labellingunsegmentedsequencedatawithrecurrentneuralnetworks. InPro- ceedings of the 23rd International Conference on Machine Learning, ICML ’06, pp. 369–376, New York, NY, USA, 2006a. Association for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844.1143891. URLhttps://doi.org/10.1145/1143844.1143891. Alex Graves, SantiagoFerna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber. Connectionisttem- poral classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedingsofthe23rdinternationalconferenceonMachinelearning,pp.369–376,2006b. ShoutaoGuo,ShaoleiZhang,ZhengruiMa,MinZhang,andYangFeng.Agent-simt:Agent-assisted simultaneousmachinetranslationwithlargelanguagemodels. arXivpreprintarXiv:2406.06910, 2024. YukiyaHono,KohMitsuda,TianyuZhao,KentaroMitsui,ToshiakiWakatsuki,andKeiSawada.In- tegratingpre-trainedspeechandlanguagemodelsforend-to-endspeechrecognition. InLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational LinguisticsACL2024,pp.13289–13305,Bangkok,Thailandandvirtualmeeting,August2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-acl.787. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, andAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningbymasked predictionofhiddenunits. IEEE/ACMtransactionsonaudio,speech,andlanguageprocessing, 29:3451–3460,2021. Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al. Wavllm: Towards robust and adaptive speech large languagemodel. arXivpreprintarXiv:2404.00656,2024. Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/ LJ-Speech-Dataset/,2017. Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In International Conference on Machine Learning, pp. 5530–5540.PMLR,2021. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversar- ial networks for efficient and high fidelity speech synthesis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural In- formation Processing Systems, volume 33, pp. 17022–17033. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf. Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics,9:1336–1354,2021. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang,andTatsunoriB.Hashimoto. Alpacaeval: Anautomaticevaluatorofinstruction-following models. https://github.com/tatsu-lab/alpaca_eval,52023. DanLiu, MenggeDu, XiaoxiLi, YaLi, andEnhongChen. Crossattentionaugmentedtransducer networks for simultaneous translation. In Proceedings of the 2021 Conference on Empirical MethodsinNaturalLanguageProcessing,pp.39–55,2021. Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, et al. Stacl: Simultaneous translation with im- plicit anticipation and controllable latency using prefix-to-prefix framework. arXiv preprint arXiv:1810.08398,2018. 12 Preprint Xutai Ma, Juan Pino, James Cross, Liezl Puzon, and Jiatao Gu. Monotonic multihead attention. arXivpreprintarXiv:1909.12406,2019. Zhengrui Ma, Shaolei Zhang, Shoutao Guo, Chenze Shao, Min Zhang, and Yang Feng. Non- autoregressive streaming transformer for simultaneous translation. In Proceedings of the 2023 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.5177–5190,2023. Zhengrui Ma, Qingkai Fang, Shaolei Zhang, Shoutao Guo, Yang Feng, and Min Zhang. A non-autoregressivegenerationframeworkforend-to-endsimultaneousspeech-to-anytranslation. arXivpreprintarXiv:2406.06937,2024a. ZiyangMa,YakunSong,ChenpengDu,JianCong,ZhuoChen,YupingWang,YuxuanWang,and XieChen. Languagemodelcanlistenwhilespeaking. arXivpreprintarXiv:2408.02622,2024b. ZiyangMa,GuanrouYang,YifanYang,ZhifuGao,JiamingWang,ZhihaoDu,FanYu,QianChen, Siqi Zheng, Shiliang Zhang, et al. An embarrassingly simple approach for llm with strong asr capacity. arXivpreprintarXiv:2402.08846,2024c. Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoˆıt Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. Generative spoken dialogue language modeling. Transactions of the Association for Com- putational Linguistics, 11:250–266, 2023. doi: 10.1162/tacl a 00545. URL https:// aclanthology.org/2023.tacl-1.15. OpenAI. Introducingchatgpt,2022. URLhttps://openai.com/blog/chatgpt. OpenAI. Hellogpt-4o,2024. URLhttps://openai.com/index/hello-gpt-4o/. SaraPapi,MatteoNegri,andMarcoTurchi.Attentionasaguideforsimultaneousspeechtranslation. arXivpreprintarXiv:2212.07850,2022. Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab- delrahmanMohamed,andEmmanuelDupoux. SpeechResynthesisfromDiscreteDisentangled Self-SupervisedRepresentations. InProc.Interspeech2021,2021. AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever. Robustspeechrecognitionvialarge-scaleweaksupervision. InInternationalconferenceonma- chinelearning,pp.28492–28518.PMLR,2023. ColinRaffel,Minh-ThangLuong,PeterJLiu,RonJWeiss,andDouglasEck.Onlineandlinear-time attentionbyenforcingmonotonicalignments. InInternationalconferenceonmachinelearning, pp.2837–2846.PMLR,2017. Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zala´n Borsos, Fe´lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: Alargelanguagemodelthatcanspeakandlisten. arXivpreprintarXiv:2306.12925, 2023. Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hiroshi Saruwatari. Utmos: Utokyo-sarulabsystemforvoicemoschallenge2022. InInterspeech2022, pp.4521–4525,2022. doi: 10.21437/Interspeech.2022-439. FrankSeide,MorrieDoulaty,YangyangShi,YasheshGaur,JuntengJia,andChunyangWu. Speech reallm–real-timestreamingspeechrecognitionwithmultimodalllmsbyteachingtheflowoftime. arXivpreprintarXiv:2406.09569,2024. Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi. Llasm: Large language and speech model. arXiv preprint arXiv:2308.15930, 2023. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=14rn7HpKVk. 13 Preprint Yun Tang, Anna Sun, Hirofumi Inaguma, Xinyue Chen, Ning Dong, Xutai Ma, Paden Tomasello, and Juan Pino. Hybrid transducer and attention based encoder-decoder modeling for speech- to-text tasks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pp.12441–12455,2023. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca,2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad- vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/ file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. ChenWang,MinpengLiao,ZhongqiangHuang,JinliangLu,JunhongWu,YuchenLiu,Chengqing Zong, and Jiajun Zhang. Blsp: Bootstrapping language-speech pre-training via behavior align- mentofcontinuationwriting. arXivpreprintarXiv:2309.00916,2023a. ChengyiWang,SanyuanChen,YuWu,ZiqiangZhang,LongZhou,ShujieLiu,ZhuoChen,Yanqing Liu,HuamingWang,JinyuLi,etal. Neuralcodeclanguagemodelsarezero-shottexttospeech synthesizers. arXivpreprintarXiv:2301.02111,2023b. Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li,andFuruWei. Viola: Unifiedcodeclanguagemodelsforspeechrecognition,synthesis, and translation. arXivpreprintarXiv:2305.16107,2023c. JianWu,YasheshGaur,ZhuoChen,LongZhou,YimengZhu,TianruiWang,JinyuLi,ShujieLiu, BoRen, LinquanLiu,etal. Ondecoder-onlyarchitectureforspeech-to-textandlargelanguage model integration. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),pp.1–8.IEEE,2023. WenyiYu,ChangliTang,GuangzhiSun,XianzhaoChen,TianTan,WeiLi,LuLu,ZejunMa,and Chao Zhang. Connecting speech encoder and large language model for asr. In ICASSP 2024- 2024IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp. 12637–12641.IEEE,2024. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. SpeechGPT:Empoweringlargelanguagemodelswithintrinsiccross-modalconversationalabil- ities. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15757–15773, Singapore, December 2023. As- sociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.1055. URL https://aclanthology.org/2023.findings-emnlp.1055. Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechgpt-gen: Scalingchain-of-informationspeechgeneration. arXivpreprintarXiv:2401.13527,2024a. ShaoleiZhangandYangFeng. Unifiedsegment-to-segmentframeworkforsimultaneoussequence generation. AdvancesinNeuralInformationProcessingSystems,36,2024. Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, and Yang Feng. Stream- speech: Simultaneous speech-to-speech translation with multi-task learning. arXiv preprint arXiv:2406.03049,2024b. 14 Preprint A PROMPT PromptforChatGPTScoring(Model:GPT-4o) Ineedyourhelptoevaluatetheperformanceofseveralmodelsinthespeechinteractionscenario.Themod- elswillreceiveaspeechinputfromtheuser,whichtheyneedtounderstandandrespondtowithaspeech output. Yourtaskistoratethemodel’sresponsesbasedontheprovideduserinputtranscription[Instruc- tion]andthemodel’soutputtranscription[Response].Pleaseevaluatetheresponsefromtwoperspectives: contentandstyle,andprovideascoreforeachonascaleof1to5. Content(1-5points): 1point:Theresponseislargelyirrelevant,incorrect,orfailstoaddresstheuser’squery.Itmaybeoff-topic orprovideincorrectinformation. 2 points: The response is somewhat relevant but lacks accuracy or completeness. It may only partially answertheuser’squestionorincludeextraneousinformation. 3points:Theresponseisrelevantandmostlyaccurate,butitmaylackconcisenessorincludeunnecessary detailsthatdon’tcontributetothemainpoint. 4points: Theresponseisrelevant,accurate,andconcise,providingaclearanswertotheuser’squestion withoutunnecessaryelaboration. 5points: Theresponseisexceptionallyrelevant,accurate,andtothepoint. Itdirectlyaddressestheuser’s queryinahighlyeffectiveandefficientmanner,providingexactlytheinformationneeded. Style(1-5points): 1point: Theresponseispoorlysuitedforspeechinteraction,possiblyincludingstructuredelementslike listsorbeingoverlycomplex,disjointed,ordifficulttounderstand. 2points:Theresponseissomewhatsuitablebutmaybetoolong,tooshort,orawkwardlyphrased,making itlesseffectiveinaspeechinteractioncontext. 3points:Theresponseisgenerallysuitableforspeechinteraction,butitmayhaveminorissueswithlength, clarity,orfluencythatdetractslightlyfromtheoveralleffectiveness. 4points: Theresponseiswell-suitedforspeechinteraction,withappropriatelength,clearlanguage,anda naturalflow.Itiseasytounderstandwhenspokenaloud. 5points: Theresponseisperfectlysuitedforspeechinteraction. Itistheideallength, highlyclear, and flowsnaturally,makingiteasytofollowandunderstandwhenspoken. Belowarethetranscriptionofuser’sinstructionandmodels’response: ###[Instruction]:{instruction} ###[Response]:{response} Afterevaluating,pleaseoutputthescoresinJSONformat:{“content”:contentscore,“style”:stylescore}. Youdon’tneedtoprovideanyexplanations. PromptforInstructionRewriting(Model:Llama-3-70B-Instruct) Belowisaninstructiondatacontainingtheuser’sinstruction. Iwouldliketogenerateaspeechversionof thisinstructionfortrainingalargelanguagemodelthatsupportsspeechinput.Therefore,pleaserewritemy instructiondataaccordingtothefollowingrequirements: 1. Modifytheinstructiontosimulatehumanspeech,addingfillersasappropriate(butnottoomany‘you know’,‘like’,etc.). 2. ThequestionshouldnotcontaincontentthatcannotbesynthesizedbytheTTSmodel. Numbersshould bewritteninEnglishwordsratherthanArabicnumerals. 3.Thequestionshouldberelativelybriefwithoutexcessiveverbiage. [instruction]:{instruction} PleaseoutputinJSONformatasfollows:{“question”:{question}}. 15 Preprint PromptforResponseGeneration(Model:Llama-3-70B-Instruct) Belowisthetranscribedtextofauser’sspeechquery.Pleaseprovidearesponsetothisquestion,whichwill beconvertedtospeechusingTTS.Pleasefollowtheserequirementsforyourresponse: 1. YourresponseshouldnotcontaincontentthatcannotbesynthesizedbytheTTSmodel,suchasparen- theses,orderedlists,etc.NumbersshouldbewritteninEnglishwordsratherthanArabicnumerals. 2.Yourresponseshouldbeveryconciseandtothepoint,avoidinglengthyexplanations. [instruction]:{instruction} PleaseoutputinJSONformatasfollows:{“response”:{response}}. 16 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "pdf_path = r\"C:\\Users\\dlrlg\\Documents\\paper\\paper_1.pdf\"\n",
    "# 미리 정의된 목차\n",
    "all_sections = [\"1 INTRODUCTION\", \"2 MODEL: LLAMA-OMNI\", \"3 CONSTRUCTION OF SPEECH INSTRUCTION DATA: INSTRUCTS2S-200K\", \"4 EXPERIMENTS\", \"5 RELATED WORK\"]\n",
    "\n",
    "# 딕셔너리 초기화\n",
    "contents_dict = {section: \"\" for section in all_sections}\n",
    "current_key = None\n",
    "\n",
    "# PDF 파일 열기\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "\n",
    "                # 현재 키가 정의된 목차 중 하나인지 확인\n",
    "                if line in contents_dict:\n",
    "                    current_key = line  # 현재 목차 키 설정\n",
    "                elif current_key:\n",
    "                    # 현재 키에 해당하는 내용 추가\n",
    "                    contents_dict[current_key] += line + \" \"\n",
    "\n",
    "# 결과 출력\n",
    "for key, value in contents_dict.items():\n",
    "    if value:  # 내용이 있는 경우에만 출력\n",
    "        print(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDtSuAX-R-dNW3YEHvaJ93T6RwQ-VWOo9k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "API_KEY = os.getenv('GOOGLE_GEMINI_API_KEY')\n",
    "print(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-1.5-flash-002\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for item in genai.list_models():\n",
    "    print(item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_config = genai.GenerationConfig(temperature=1, response_mime_type=\"application/json\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"\"\"\n",
    "    f{contents_dict[1 INTRODUCTION]} 이 글을 한국말로 요약해줘\n",
    "                                  \n",
    "                                  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1 INTRODUCTION', '2 MODEL: LLAMA-OMNI', '3 CONSTRUCTION OF SPEECH INSTRUCTION DATA: INSTRUCTS2S-200K', '4 EXPERIMENTS', '5 RELATED WORK'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"1. Introduction\" 키의 내용을 가져오기\n",
    "introduction_content = contents_dict.get(\"3 CONSTRUCTION OF SPEECH INSTRUCTION DATA: INSTRUCTS2S-200K\", \"\")\n",
    "# 요약 요청\n",
    "response = model.generate_content(f\"\"\"\n",
    "    {introduction_content} 이 글을 한국말로 요약해줘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLaMA-Omni를 훈련하기 위해서는 음성 지시, 텍스트 응답, 음성 응답으로 구성된 삼중항 데이터가 필요합니다. 하지만 공개적으로 이용 가능한 지시 데이터는 대부분 텍스트 형태로 제공됩니다. 따라서 기존의 텍스트 지시 데이터를 기반으로 음성 지시 데이터를 구축하는 과정을 다음과 같이 설명합니다.\\n\\n1. **지시 재작성**: 음성 입력은 텍스트 입력과 특징이 다르므로, 텍스트 지시를 다음 규칙에 따라 재작성합니다.\\n    * 자연스러운 음성 패턴을 모방하기 위해 \"안녕하세요\", \"그래서\", \"어\", \"음\" 등과 같은 적절한 채움 단어를 추가합니다.\\n    * 숫자와 같은 텍스트가 아닌 기호를 TTS에서 올바르게 합성될 수 있도록 해당 발음으로 변환합니다.\\n    * 과도한 말투 없이 상대적으로 간결하게 지시를 수정합니다. Llama-3-70B-Instruct 모델을 사용하여 이러한 규칙에 따라 지시를 재작성합니다.\\n2. **응답 생성**: 음성 상호 작용에서 텍스트 지시에서 나온 기존 응답은 직접 음성 지시 응답으로 사용하기에 적합하지 않습니다. 텍스트 기반 상호 작용에서 모델은 복잡한 문장을 사용하고 비언어적 요소(예: 순서가 있는 목록이나 괄호)를 포함하여 긴 응답을 생성하는 경향이 있습니다. 그러나 음성 상호 작용에서는 간결하지만 정보가 풍부한 응답이 일반적으로 선호됩니다. 따라서 Llama-3-70B-Instruct 모델을 사용하여 다음 규칙에 따라 음성 지시에 대한 응답을 생성합니다.\\n    * 응답에는 괄호, 순서가 있는 목록 등 TTS 모델로 합성할 수 없는 콘텐츠를 포함하지 않아야 합니다.\\n    * 응답은 매우 간결하고 요점을 명확하게 해야 하며 긴 설명을 피해야 합니다.\\n3. **음성 합성**: 음성 상호 작용에 적합한 지시와 응답을 얻은 후 TTS 모델을 사용하여 음성으로 변환해야 합니다. 지시의 경우, 합성된 음성이 더 자연스럽게 들리도록 CosyVoice-300M-SFT 모델을 사용하여 각 지시에 남성 또는 여성 음성 중 하나를 무작위로 선택합니다. 응답의 경우, LJSpeech 데이터 세트에서 훈련된 VITS 모델을 사용하여 표준 음성으로 응답을 합성합니다.\\n\\n기본적인 텍스트 지시에 대해 알파카 데이터 세트에서 약 5만 개의 지시를 수집했으며, 이는 광범위한 주제를 다룹니다. 또한 주로 세상에 대한 질문으로 구성된 울트라챗 데이터 세트에서 약 15만 개의 지시를 수집했습니다. 울트라챗은 대규모 다회전 대화 데이터 세트이지만, 처음 15만 개 항목만 선택하고 첫 번째 라운드 지시만 사용합니다.\\n\\n위의 데이터 세트와 데이터 처리 파이프라인을 사용하여 궁극적으로 InstructS2S-200K라고 하는 20만 개의 음성 지시 데이터를 얻었습니다.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Largelanguagemodels(LLMs),representedbyChatGPT(OpenAI,2022),havebecomepowerful general-purposetasksolvers,capableofassistingpeopleindailylifethroughconversationalinter- actions. However, most LLMs currently only support text-based interactions, which limits their applicationinscenarioswheretextinputandoutputarenotideal. Recently,theemergenceofGPT- 4o(OpenAI,2024)hasmadeitpossibletointeractwithLLMsthroughspeech,respondingtouser’s instruction with extremely low latency and significantly enhancing the user experience. However, thereisstillalackofexplorationintheopen-sourcecommunityonbuildingsuchspeechinteraction modelsbasedonLLMs. Therefore,howtoachievelow-latencyandhigh-qualityspeechinteraction withLLMsisapressingchallengethatneedstobeaddressed. The simplest way to enable speech interaction with LLMs is through a cascaded system based on automaticspeechrecognition(ASR)andtext-to-speech(TTS)models,wheretheASRmodeltran- scribestheuser’sspeechinstructionintotext, andtheTTSmodelsynthesizestheLLM’sresponse intospeech. However, sincethecascadedsystemsequentiallyoutputsthetranscribedtext, textre- sponse, and speech response, the overall system tends to have higher latency. In contrast, some multimodal speech-language models have been proposed (Zhang et al., 2023; Rubenstein et al., 2023),whichdiscretizespeechintotokensandextendtheLLM’svocabularytosupportspeechin- putandoutput. Suchspeech-languagemodelstheoreticallycangeneratespeechresponsesdirectly ∗CorrespondingAuthor:YangFeng. 1Codeandmodelareavailableathttps://github.com/ictnlp/LLaMA-Omni. 1 4202 peS 01 ]LC.sc[ 1v66660.9042:viXra Preprint simultaneous LLaMA-Omni generation (Hey! Can you give me some Certainly! Writing a high-quality NLP paper requires … advices on writing NLP papers?) Figure1:LLaMA-Omnicansimultaneouslygeneratetextandspeechresponsesbasedonthespeech instruction,withextremelylowresponselatency. fromspeechinstructionswithoutproducingintermediatetext,therebyachievingextremelylowre- sponse latency. However, in practice, direct speech-to-speech generation can be challenging due to thecomplex mappinginvolved, soit is commonto generateintermediate textto achieve higher generationquality(Zhangetal.,2023),althoughthissacrificessomeresponselatency. Inthispaper,weproposeanovelmodelarchitecture,LLaMA-Omni,whichenableslow-latencyand high-qualityinteractionwithLLMs. LLaMA-Omniconsistsofaspeechencoder,aspeechadaptor, an LLM, and a streaming speech decoder. The user’s speech instruction is encoded by the speech encoder followed by the speech adaptor, and then input into the LLM. The LLM decodes the text response directly from the speech instruction, without first transcribing the speech into text. The speechdecoderisanon-autoregressive(NAR)streamingTransformer(Maetal.,2023),whichtakes theoutputhiddenstatesoftheLLMasinputandusesconnectionisttemporalclassification(CTC; Gravesetal.,2006a)topredictthesequenceofdiscreteunitscorrespondingtothespeechresponse. Duringinference, astheLLM autoregressivelygeneratesthetextresponse, thespeech decodersi- multaneouslygeneratesthecorrespondingdiscreteunits. Tobetteralignwiththecharacteristicsof speechinteractionscenarios,weconstructadatasetnamedInstructS2S-200Kbyrewritingexisting text instruction data and performing speech synthesis. Experimental results show that LLaMA- Omnicansimultaneouslygeneratehigh-qualitytextandspeechresponseswithalatencyaslowas 226ms. Additionally,comparedtopreviousspeech-languagemodelslikeSpeechGPT(Zhangetal., 2023),LLaMA-Omnisignificantlyreducestherequiredtrainingdataandcomputationalresources, enablingtheefficientdevelopmentofpowerfulspeechinteractionmodelsbasedonthelatestLLMs. '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_dict[\"1 INTRODUCTION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
